{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebecdc09",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sofiavacaaa/nlp-lab-language-models/blob/main/language_model_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30626a2",
   "metadata": {
    "id": "e30626a2"
   },
   "source": [
    "# NLP Lab: Language Models\n",
    "\n",
    "In this lab, we will build the main components of the GPT-2 model and train a small model on poems by Victor Hugo.\n",
    "\n",
    "The questions are included in this notebook. To run the training, you will need to modify the `gpt_single_head.py` file, which is also available in the Git repository.\n",
    "\n",
    "## Data\n",
    "\n",
    "The training data consists of a collection of poems by Victor Hugo, sourced from [gutenberg.org](https://www.gutenberg.org/). The dataset is available in the `data` directory.\n",
    "\n",
    "To reduce model complexity, we will model the text at the character level. Typically, language models process sequences of subwords using [tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary) such as BPE, SentencePiece, or WordPiece.\n",
    "\n",
    "#### Questions:\n",
    "- Using [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter), display the number of unique characters in the text and the frequency of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4d4ab91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4d4ab91",
    "outputId": "6f4fd3f2-a758-49fe-f489-b788102569a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the file: 285222\n",
      "Number of character in counter: 285222\n",
      "101 different characters\n",
      "Counter({' ': 49127, 'e': 30253, 's': 17987, 'u': 14254, 'r': 14223, 't': 14071, 'a': 14048, 'n': 13725, 'i': 12828, 'o': 12653, 'l': 11638, '\\n': 8102, 'm': 6495, 'd': 6375, ',': 6077, 'c': 5074, 'p': 4206, \"'\": 3820, 'v': 3492, 'é': 2943, 'b': 2783, 'f': 2772, 'h': 2221, 'q': 1956, 'g': 1790, '.': 1420, 'x': 1154, 'L': 1147, '!': 1121, 'E': 1074, ';': 1043, '-': 1020, 'j': 890, 'D': 764, 'è': 725, 'à': 706, 'y': 660, 'I': 627, 'ê': 605, 'C': 593, 'S': 545, 'A': 530, 'Q': 503, 'z': 482, 'J': 471, 'O': 450, 'T': 441, 'P': 435, '?': 388, 'V': 383, 'â': 381, 'N': 362, 'M': 344, 'ù': 298, ':': 294, 'R': 240, 'î': 214, 'U': 208, 'ô': 159, 'X': 150, '1': 146, 'H': 116, 'F': 114, '5': 111, '8': 93, 'B': 78, '«': 74, 'É': 70, '»': 69, 'G': 67, '4': 64, 'û': 62, '3': 47, 'ç': 34, 'À': 33, 'ë': 32, 'ï': 31, '2': 30, '·': 26, 'Ê': 24, '6': 23, '7': 23, 'Ô': 19, '9': 19, 'È': 11, 'k': 10, '0': 10, '_': 8, 'Z': 7, 'Æ': 4, '[': 4, ']': 4, 'w': 3, 'K': 3, 'Y': 3, 'Ë': 2, '(': 2, ')': 2, 'Â': 2, 'Î': 1, 'W': 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "with open('hugo_contemplations.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Number of characters in the file: {len(text)}')\n",
    "\n",
    "# Count character frequency\n",
    "counter = collections.Counter(text)\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Get unique characters\n",
    "chars = counter.keys()\n",
    "\n",
    "print (f'Number of character in counter: {sum(counter.values())}')\n",
    "print (f'{len(chars)} different characters')\n",
    "print (counter)\n",
    "\n",
    "#We count the number of unique characters in the text and the frequency of each character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b661f",
   "metadata": {
    "id": "d80b661f"
   },
   "source": [
    "### Encoding / Decoding  \n",
    "\n",
    "To transform the text into a vector for the neural network, each character must be encoded as an integer.  \n",
    "\n",
    "The following functions perform the encoding and decoding of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d9c974d",
   "metadata": {
    "id": "8d9c974d"
   },
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "# Character to integer\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "# Integer to character\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "#Encoding a String into Integers\n",
    "# for example encode(\"abc\")  the output  would be [0,1,2]\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: transform a string into a list of integers\n",
    "\n",
    "# Decoding integers back into a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: transform a list of integers into a string\n",
    "# for example dencode([0,1,2])  the output  would be \"a, b, c\"\n",
    "\n",
    "# test that your encoder/decoder is coherent\n",
    "# Use of assert to check that the decoded text matches the original\n",
    "testString = \"\\nDemain, dès l'aube\"\n",
    "assert decode(encode (testString)) ==  testString\n",
    "\n",
    "#no assertion error so the system of decoding and encoding works\n",
    "\n",
    "# Why is this useful?\n",
    "# Neural Networks and Machine Learning Models don't understand text; they work with numbers.\n",
    "# This character encoding helps convert text to numerical form so that models can process it.\n",
    "# The decoding ensures that the model’s output can be converted back to human-readable text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a633d",
   "metadata": {
    "id": "bf2a633d"
   },
   "source": [
    "### Train/Validation Split  \n",
    "\n",
    "Since the goal is to predict poems, the lines should not be shuffled randomly. Instead, we must preserve the order of the lines in the text and take only the first 90% for training, while using the remaining 10% to monitor learning.  \n",
    "\n",
    "#### Questions:  \n",
    "- Split the data into `train_data` (90%) and `val_data` (10%) using slicing on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf5b7420",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf5b7420",
    "outputId": "169b076d-743f-4642-ae06-6f7998bb0634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: torch.Size([256699])\n",
      "Validation data size: torch.Size([28523])\n"
     ]
    }
   ],
   "source": [
    "# We want to predict poems we preserve the order of the lines\n",
    "import torch\n",
    "# Train and validation splits\n",
    "# Converting the text to numbers sequence\n",
    "# We keep the order with tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "## YOUR CODE HERE\n",
    "# first 90% characters will be train, rest validation\n",
    "# len(data) is the number of characters\n",
    "split_index = int(0.9 * len(data))\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_data = data[:split_index]  # First 90% for training\n",
    "val_data = data[split_index:]    # Remaining 10% for validation\n",
    "\n",
    "# Print sizes to verify\n",
    "# This shows the number of characters in each set\n",
    "print(f\"Train data size: {train_data.shape}\")\n",
    "print(f\"Validation data size: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa498280",
   "metadata": {
    "id": "aa498280"
   },
   "source": [
    "### Context  \n",
    "\n",
    "The language model has a parameter that defines the maximum context size to consider when predicting the next character. This context is called `block_size`. The training data consists of sequences of consecutive characters, randomly sampled from the training set, with a length of `block_size`.  \n",
    "\n",
    "If the starting character of the sequence is `i`, then the context sequence is:  \n",
    "```python\n",
    "x = data[i:i+block_size]\n",
    "```\n",
    "And the target value to predict at each position in the context is the next character:  \n",
    "```python\n",
    "y = data[i+1:i+block_size+1]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97a262bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97a262bf",
    "outputId": "60e4c72a-b569-48b3-9acb-e462fbd9e8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([211767])\n",
      "context is >q< target is >u<\n",
      "context is >qu< target is >e<\n",
      "context is >que< target is > <\n",
      "context is >que < target is >l<\n",
      "context is >que l< target is >'<\n",
      "context is >que l'< target is >a<\n",
      "context is >que l'a< target is >i<\n",
      "context is >que l'ai< target is >l<\n"
     ]
    }
   ],
   "source": [
    "#maximum context size to consider when predicting the next character\n",
    "block_size = 8\n",
    "# when predicting the next character, the model will look at up to 8 previous characters.\n",
    "\n",
    "# Choose a Random Starting Index (i)\n",
    "i  = torch.randint(len(data) - block_size, (1,))\n",
    "print (i)\n",
    "\n",
    "# Extract Training Data (x) and Target Data (y)\n",
    "x = train_data[i:i+block_size]\n",
    "y = train_data[i+1:i+1+block_size]\n",
    "\n",
    "# Iterating Over the Context\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print (f'context is >{decode(context.tolist())}< target is >{decode([target.tolist()])}<')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de81464",
   "metadata": {
    "id": "0de81464"
   },
   "source": [
    "### Defining Batches  \n",
    "\n",
    "The training batches consist of multiple character sequences randomly sampled from `train_data`. To randomly select a sequence for the batch, we need to randomly pick a starting point in `train_data` and extract the following `block_size` characters. When selecting the starting point, ensure that there are enough characters remaining after it to form a full sequence of `block_size` characters.  \n",
    "\n",
    "#### Questions:  \n",
    "- Create the batches `x` by selecting `batch_size` sequences of length `block_size` starting from a randomly chosen index `i`. Stack the examples using `torch.stack`.  \n",
    "- Create the batches `y` by adding the next character following each sequence in `x`. Stack the examples using `torch.stack`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9be91965",
   "metadata": {
    "id": "9be91965"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "torch.manual_seed(2023)\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ## YOUR CODE HERE\n",
    "    # select batch_size starting points in the data, store them in a list called starting_points\n",
    "    starting_points = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # x is the sequence of integer starting at each straing point and of length block_size\n",
    "    x = torch.stack([data[i:i+block_size] for i in starting_points])\n",
    "    \n",
    "    # y is the character after each starting position\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in starting_points])\n",
    "    \n",
    "    ###\n",
    "    # send data and target to device\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507313b",
   "metadata": {
    "id": "6507313b"
   },
   "source": [
    "### First Model: A Bigram Model  \n",
    "\n",
    "The first model we will implement is a bigram model. It predicts the next character based only on the current character. This model can be stored in a simple matrix: for each character (row), we store the probability distribution over all possible next characters (columns). This can be implemented using a simple [`Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer in PyTorch.  \n",
    "\n",
    "#### Questions:  \n",
    "- In the constructor, define an Embedding layer of size `vocab_size × vocab_size`.  \n",
    "- In the `forward` method, apply the embedding layer to the batch of indices (`x`).  \n",
    "- In the `forward` method, define the loss as `cross_entropy` between the predictions and the target (`y`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b7a7478",
   "metadata": {
    "id": "9b7a7478"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# use a gpu if we have one\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # we use a simple vocab_size times vocab_size tensor to store the probabilities\n",
    "        # of each token given a single token as context in nn.Embedding\n",
    "        # YOUR CODE HERE\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        ##\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (Batch,Time) tensor of integers\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        ##\n",
    "\n",
    "        # don't compute loss if we don't have targets\n",
    "        if  targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # change the shape of the logits and target to match what is needed for CrossEntropyLoss\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "            Batch, Time, Channels = logits.shape\n",
    "            logits = logits.view(Batch*Time, Channels)\n",
    "            targets = targets.view(Batch*Time)\n",
    "\n",
    "            # negative log likelihood between prediction and target\n",
    "            # YOUR CODE HERE\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "# send the model to device\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3cffb",
   "metadata": {
    "id": "f0e3cffb"
   },
   "source": [
    "### Model Before Training  \n",
    "\n",
    "At this stage, the model has not yet been trained—it has only been initialized. However, we can already compute the loss on a random batch. Since the weights are initialized with a normal distribution \\( N(0,1) \\) for each dimension, the expected loss after initialization should be close to `-ln(1/vocab_size)`, as the entropy is maximal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62343cbc",
   "metadata": {
    "id": "62343cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 101])\n",
      "Expected loss 4.61512051684126\n",
      "Computed loss 5.199841499328613\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "print (logits.shape)\n",
    "print (f'Expected loss {-math.log(1.0/vocab_size)}')\n",
    "print (f'Computed loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d44b50",
   "metadata": {
    "id": "d0d44b50"
   },
   "source": [
    "### Using the Model for Prediction  \n",
    "\n",
    "To use the model for prediction, we need to provide an initial character to start the sequence—this is called the prompt. In our case, we can initialize the generation with the newline character (`\\n`) to start a new sentence.  \n",
    "\n",
    "#### Questions:  \n",
    "- Create a prompt as a tensor of size `(1,1)` containing the integer corresponding to the character `\\n`.  \n",
    "- Generate a sequence of 100 characters from this prompt using the functions `m.generate` and `decode`.  \n",
    "- How does the generated sentence look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "320d3ead",
   "metadata": {
    "id": "320d3ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "\n",
      "L«[8jK-RîT0a6;LH! vùÂW6alUï;Pd1ÆêCëËù ;LfO;QyâÎfMHP9ëeo3NPiQjXçE4éjg9NdXguXaWH!;VÀ-Wéj4ÊAÉ1KHdBèÀx»À\n"
     ]
    }
   ],
   "source": [
    "print (encode(['\\n']))\n",
    "## YOUR CODE HERE\n",
    "\n",
    "prompt = torch.tensor([encode('\\n')], dtype=torch.long).to(device)  # shape: (1, 1)\n",
    "\n",
    "generated_indices = m.generate(prompt, max_new_tokens=100)\n",
    "\n",
    "\n",
    "generated_text = decode(generated_indices[0].tolist())\n",
    "\n",
    "\n",
    "print(generated_text)\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630d8c3",
   "metadata": {
    "id": "3630d8c3"
   },
   "source": [
    "### Training  \n",
    "\n",
    "For training, we use the [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer with a learning rate of `1e-3`. Each training iteration consists of the following steps:  \n",
    "\n",
    "- Generate a batch  \n",
    "- Apply the neural network (forward pass) and compute the loss: `model(xb, yb)`  \n",
    "- Compute the gradient (after resetting accumulated gradients): `loss.backward()`  \n",
    "- Update the parameters: `optimizer.step()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05831b58",
   "metadata": {
    "id": "05831b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.1666, val loss 5.1454\n",
      "step 10: train loss 5.1687, val loss 5.1316\n",
      "step 20: train loss 5.1383, val loss 5.0939\n",
      "step 30: train loss 5.1102, val loss 5.0613\n",
      "step 40: train loss 5.1236, val loss 5.0817\n",
      "step 50: train loss 5.1290, val loss 5.1232\n",
      "step 60: train loss 5.1052, val loss 4.9876\n",
      "step 70: train loss 5.1474, val loss 5.1393\n",
      "step 80: train loss 5.1460, val loss 5.1129\n",
      "step 90: train loss 5.2004, val loss 4.9291\n"
     ]
    }
   ],
   "source": [
    "max_iters = 100\n",
    "batch_size = 4\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 20\n",
    "\n",
    "@torch.no_grad() # no gradient is computed here\n",
    "def estimate_loss():\n",
    "    \"\"\" Estimate the loss on eval_iters batch of train and val sets.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# re-create the model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512395a0",
   "metadata": {
    "id": "512395a0"
   },
   "source": [
    "Once the network has been trained for 100 iterations, we can generate a sequence of characters.  \n",
    "\n",
    "#### Questions:  \n",
    "- What is the effect of training?  \n",
    "- Increase the number of iterations to 1,000 and then to 10,000. Note the obtained loss and the generated sentence. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66e6a0",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "At first, the model generates random characters and the loss is high. Then as we train it the loss decreases and the model begins to learn patterns in the text, generating more coherent sequences. With the training, the characters begin to resemble real words or repeated fragments of the training text.\n",
    "\n",
    "This can be seen when comparing the results when the number of iterations used increases. As the number of iterations increases, the loss decreases and the generated text improves. At the beginning, the characters are random; after 1000 or 10000 steps, the model starts to generate more coherent fragments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4071b998",
   "metadata": {
    "id": "4071b998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QkÀWQUvMaÔ9T358uNDqÂR!ô)o)!wxJ,..ztvF ,dÊmfFSçi-aëCyÉMBO!·XN8\n",
      "·Kê!«[3ùp;îÆJ]xÎxzûMRÉ,ydkOlAJd·UlÎEJâ\n"
     ]
    }
   ],
   "source": [
    "idx = torch.ones((1,1), dtype=torch.long)*3\n",
    "print (decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6bd8f670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.2040, val loss 5.2894\n",
      "step 10: train loss 5.2010, val loss 5.2572\n",
      "step 20: train loss 5.2111, val loss 5.2807\n",
      "step 30: train loss 5.1044, val loss 5.2264\n",
      "step 40: train loss 5.1915, val loss 5.2477\n",
      "step 50: train loss 5.1461, val loss 5.2589\n",
      "step 60: train loss 5.1509, val loss 5.1745\n",
      "step 70: train loss 5.1172, val loss 5.2513\n",
      "step 80: train loss 5.0999, val loss 5.0952\n",
      "step 90: train loss 5.1154, val loss 5.1715\n",
      "step 100: train loss 5.0501, val loss 5.2135\n",
      "step 110: train loss 5.0669, val loss 5.2025\n",
      "step 120: train loss 5.0680, val loss 5.2628\n",
      "step 130: train loss 4.9593, val loss 5.1844\n",
      "step 140: train loss 5.0125, val loss 5.1807\n",
      "step 150: train loss 5.0388, val loss 5.1867\n",
      "step 160: train loss 4.9646, val loss 5.1123\n",
      "step 170: train loss 5.0312, val loss 5.2096\n",
      "step 180: train loss 5.0200, val loss 5.1708\n",
      "step 190: train loss 4.9286, val loss 5.0229\n",
      "step 200: train loss 4.9641, val loss 5.0166\n",
      "step 210: train loss 4.9099, val loss 5.1364\n",
      "step 220: train loss 4.9464, val loss 5.0277\n",
      "step 230: train loss 4.8558, val loss 5.1992\n",
      "step 240: train loss 4.9025, val loss 5.0624\n",
      "step 250: train loss 4.9013, val loss 5.0443\n",
      "step 260: train loss 5.0589, val loss 5.0786\n",
      "step 270: train loss 4.9076, val loss 5.0547\n",
      "step 280: train loss 4.9449, val loss 5.0062\n",
      "step 290: train loss 4.8240, val loss 5.0390\n",
      "step 300: train loss 4.8830, val loss 4.9929\n",
      "step 310: train loss 4.8845, val loss 4.9581\n",
      "step 320: train loss 4.9666, val loss 4.9275\n",
      "step 330: train loss 4.9046, val loss 4.9205\n",
      "step 340: train loss 4.7925, val loss 5.0319\n",
      "step 350: train loss 4.8670, val loss 4.9561\n",
      "step 360: train loss 4.8964, val loss 5.0056\n",
      "step 370: train loss 4.7865, val loss 4.8988\n",
      "step 380: train loss 4.8598, val loss 4.8744\n",
      "step 390: train loss 4.7726, val loss 4.9377\n",
      "step 400: train loss 4.7940, val loss 5.0045\n",
      "step 410: train loss 4.8443, val loss 4.9160\n",
      "step 420: train loss 4.8425, val loss 4.9172\n",
      "step 430: train loss 4.7920, val loss 4.8547\n",
      "step 440: train loss 4.7369, val loss 4.9434\n",
      "step 450: train loss 4.7928, val loss 4.8134\n",
      "step 460: train loss 4.8189, val loss 4.8799\n",
      "step 470: train loss 4.8170, val loss 4.9045\n",
      "step 480: train loss 4.7795, val loss 4.8410\n",
      "step 490: train loss 4.7544, val loss 4.8568\n",
      "step 500: train loss 4.7337, val loss 4.7836\n",
      "step 510: train loss 4.7574, val loss 4.7581\n",
      "step 520: train loss 4.7205, val loss 4.8307\n",
      "step 530: train loss 4.7276, val loss 4.8725\n",
      "step 540: train loss 4.7352, val loss 4.8497\n",
      "step 550: train loss 4.6255, val loss 4.8265\n",
      "step 560: train loss 4.7247, val loss 4.7328\n",
      "step 570: train loss 4.7343, val loss 4.7928\n",
      "step 580: train loss 4.6635, val loss 4.8022\n",
      "step 590: train loss 4.6738, val loss 4.7686\n",
      "step 600: train loss 4.6450, val loss 4.7696\n",
      "step 610: train loss 4.6750, val loss 4.7334\n",
      "step 620: train loss 4.6919, val loss 4.7245\n",
      "step 630: train loss 4.5707, val loss 4.7106\n",
      "step 640: train loss 4.6207, val loss 4.7553\n",
      "step 650: train loss 4.6644, val loss 4.7386\n",
      "step 660: train loss 4.5691, val loss 4.7449\n",
      "step 670: train loss 4.6464, val loss 4.7313\n",
      "step 680: train loss 4.5734, val loss 4.6897\n",
      "step 690: train loss 4.6235, val loss 4.6317\n",
      "step 700: train loss 4.5706, val loss 4.7146\n",
      "step 710: train loss 4.5480, val loss 4.6567\n",
      "step 720: train loss 4.5741, val loss 4.7197\n",
      "step 730: train loss 4.5666, val loss 4.6917\n",
      "step 740: train loss 4.5517, val loss 4.7265\n",
      "step 750: train loss 4.5163, val loss 4.5853\n",
      "step 760: train loss 4.5601, val loss 4.6573\n",
      "step 770: train loss 4.5206, val loss 4.4891\n",
      "step 780: train loss 4.5521, val loss 4.5698\n",
      "step 790: train loss 4.5551, val loss 4.6586\n",
      "step 800: train loss 4.5157, val loss 4.5437\n",
      "step 810: train loss 4.4635, val loss 4.5679\n",
      "step 820: train loss 4.4340, val loss 4.5941\n",
      "step 830: train loss 4.4163, val loss 4.6298\n",
      "step 840: train loss 4.5123, val loss 4.5970\n",
      "step 850: train loss 4.4635, val loss 4.5622\n",
      "step 860: train loss 4.5215, val loss 4.6937\n",
      "step 870: train loss 4.4402, val loss 4.5224\n",
      "step 880: train loss 4.4499, val loss 4.5881\n",
      "step 890: train loss 4.4556, val loss 4.6570\n",
      "step 900: train loss 4.4059, val loss 4.5155\n",
      "step 910: train loss 4.3551, val loss 4.6467\n",
      "step 920: train loss 4.4149, val loss 4.5505\n",
      "step 930: train loss 4.3554, val loss 4.5864\n",
      "step 940: train loss 4.3653, val loss 4.4631\n",
      "step 950: train loss 4.3821, val loss 4.3991\n",
      "step 960: train loss 4.3406, val loss 4.5623\n",
      "step 970: train loss 4.3408, val loss 4.4329\n",
      "step 980: train loss 4.3926, val loss 4.5884\n",
      "step 990: train loss 4.3104, val loss 4.4408\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "batch_size = 4\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 20\n",
    "\n",
    "@torch.no_grad() # no gradient is computed here\n",
    "def estimate_loss():\n",
    "    \"\"\" Estimate the loss on eval_iters batch of train and val sets.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# re-create the model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee14056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x3P;(9Î6qd4éIoIxJDA.iè1gâïôBE9nù»G!wËGÆy?nïÆîNmuçKpoakxJBNwsWDÊRnù:'éë·XXCuO0ÉÀQqDANgXÉ ÎèâîvLSZxXèvY9R8È)x«ÊKÀû_ewôoGÆaI[àà-6çN[Ta èPLS3.i!u[0! :êZ]:UèÊt PoxH.ilu?n mtS:2yrîN6d86!ïïp\n",
      "ÆDtrqzCMM\n",
      "·hçiq]É-eaktémîn.çM'oYbvE)Ê:ù'!Wveï8hMôëç»?âbM'K 1BBÀve0oaîILË6ÉÂ!e.uè\n",
      "X)Z6Ban[g8Ô1gVbôqR3ïôgèÊÀVU.]kûC-9c\n",
      "1î·FiO0;F]\n",
      ",Af·Î,8Éfp0FS:\n",
      "ôysWusWâ!»EfêbJM?(paVV_s,A?êbsnfwbDXnïa)Yâ!ëGhKztx·ïa]RÈç5Kil(\n",
      "N2aHKZW[l«SdÂliÉËDFxFÊuoÂ;GÆà3'àlëMopé:(ëoÔ[ÉÔp·,Éçp6êO0ÂKÉjv»2NwplpPé7â6qdÉG2y3v41f:?eootCkugJiKAbYbaBÎ)RéÈ.dGûx,,èdnLÎÎ[éYM67PÈîwVpXôbUyÊ[laWBû5FNbnv?OëTP0nÔ0!WVÉOËÔL6BôdMnÆ!éàg\n",
      "VÔL_ÆÂzxzçÂ·ÔwkF]Dûjï-N lÎ6O0t:Yjê7Ôdbs'nRKxBgvAàdyNv[guavJzStV(-3)ôRkuPeprTTPËÉLKY_sÈreR·ÎWt7!1ôxà·IoêôhK m\n",
      "ï.r\n",
      "PK,ECqÆpfde(è\n",
      "ÂXc1GFHaxs[àVÊlçÈNx«ÔÎ.TÎÈsW»?âê(pY»M'WYLÂR·o'udNmeC5ÆK8TaV8c.[4Ec.R·PEllt;gHx)3ÈâRXl«0IïCC]qcwîrn!HûJTÆùlPËÉRd2q3UAàcÉZT2h2y«g?f:7gàFùf)]?çïçé)JÈÀxT-N[bFEIQ0kCLïÀh4BYt7Âbdeûq]Hr6i-XâiEm9«MùhO6m,ÉPYàE\n",
      "Hhw8j7HVsxod1TgVÂ!.î«zË PRêïqkÔjTXVZKÀ!ë2û-î»HnLMQ3GQzm7C-MèBAv«,phV_5ëé,jERkur!utDwTàwÀËë2IV:Cûfê9)ïv\n"
     ]
    }
   ],
   "source": [
    "idx = torch.ones((1,1), dtype=torch.long)*3\n",
    "print (decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9b44f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.1914, val loss 5.0890\n",
      "step 10: train loss 5.1250, val loss 5.1386\n",
      "step 20: train loss 5.0815, val loss 5.0784\n",
      "step 30: train loss 5.1216, val loss 5.0880\n",
      "step 40: train loss 5.1510, val loss 5.1461\n",
      "step 50: train loss 5.0453, val loss 5.1165\n",
      "step 60: train loss 5.0470, val loss 5.0534\n",
      "step 70: train loss 5.0280, val loss 5.0659\n",
      "step 80: train loss 5.1429, val loss 5.0228\n",
      "step 90: train loss 5.0432, val loss 5.0047\n",
      "step 100: train loss 5.0078, val loss 5.0271\n",
      "step 110: train loss 5.0405, val loss 5.0890\n",
      "step 120: train loss 5.0518, val loss 5.0291\n",
      "step 130: train loss 5.0063, val loss 5.0069\n",
      "step 140: train loss 5.0221, val loss 4.9858\n",
      "step 150: train loss 5.0048, val loss 5.0271\n",
      "step 160: train loss 4.9869, val loss 4.9850\n",
      "step 170: train loss 5.0479, val loss 4.9975\n",
      "step 180: train loss 4.9948, val loss 4.9993\n",
      "step 190: train loss 4.9812, val loss 4.9393\n",
      "step 200: train loss 4.9561, val loss 4.9334\n",
      "step 210: train loss 4.9213, val loss 4.9039\n",
      "step 220: train loss 4.8609, val loss 4.9692\n",
      "step 230: train loss 5.0043, val loss 4.9449\n",
      "step 240: train loss 4.9376, val loss 4.9151\n",
      "step 250: train loss 4.8868, val loss 4.9254\n",
      "step 260: train loss 4.8608, val loss 4.9175\n",
      "step 270: train loss 4.8483, val loss 4.9670\n",
      "step 280: train loss 4.9423, val loss 4.8455\n",
      "step 290: train loss 4.9033, val loss 4.9349\n",
      "step 300: train loss 4.8845, val loss 4.8499\n",
      "step 310: train loss 4.8433, val loss 4.8699\n",
      "step 320: train loss 4.8910, val loss 4.8890\n",
      "step 330: train loss 4.9054, val loss 4.8446\n",
      "step 340: train loss 4.8991, val loss 4.8375\n",
      "step 350: train loss 4.7667, val loss 4.8450\n",
      "step 360: train loss 4.8191, val loss 4.7995\n",
      "step 370: train loss 4.7659, val loss 4.8458\n",
      "step 380: train loss 4.8602, val loss 4.8574\n",
      "step 390: train loss 4.8273, val loss 4.7969\n",
      "step 400: train loss 4.8542, val loss 4.8156\n",
      "step 410: train loss 4.7515, val loss 4.8000\n",
      "step 420: train loss 4.7455, val loss 4.7949\n",
      "step 430: train loss 4.7889, val loss 4.6730\n",
      "step 440: train loss 4.7469, val loss 4.7695\n",
      "step 450: train loss 4.7233, val loss 4.6816\n",
      "step 460: train loss 4.7072, val loss 4.7283\n",
      "step 470: train loss 4.7150, val loss 4.7171\n",
      "step 480: train loss 4.6937, val loss 4.7639\n",
      "step 490: train loss 4.6739, val loss 4.7180\n",
      "step 500: train loss 4.7156, val loss 4.7017\n",
      "step 510: train loss 4.6411, val loss 4.6866\n",
      "step 520: train loss 4.7150, val loss 4.6384\n",
      "step 530: train loss 4.6214, val loss 4.6453\n",
      "step 540: train loss 4.6955, val loss 4.6276\n",
      "step 550: train loss 4.6832, val loss 4.6474\n",
      "step 560: train loss 4.6138, val loss 4.6593\n",
      "step 570: train loss 4.6754, val loss 4.6545\n",
      "step 580: train loss 4.6206, val loss 4.7020\n",
      "step 590: train loss 4.6349, val loss 4.7257\n",
      "step 600: train loss 4.6326, val loss 4.5798\n",
      "step 610: train loss 4.6213, val loss 4.6691\n",
      "step 620: train loss 4.6126, val loss 4.6635\n",
      "step 630: train loss 4.5518, val loss 4.6946\n",
      "step 640: train loss 4.6171, val loss 4.5865\n",
      "step 650: train loss 4.6196, val loss 4.6331\n",
      "step 660: train loss 4.6223, val loss 4.6293\n",
      "step 670: train loss 4.5876, val loss 4.5647\n",
      "step 680: train loss 4.5688, val loss 4.5512\n",
      "step 690: train loss 4.5196, val loss 4.5745\n",
      "step 700: train loss 4.5216, val loss 4.5027\n",
      "step 710: train loss 4.5933, val loss 4.5725\n",
      "step 720: train loss 4.5073, val loss 4.4891\n",
      "step 730: train loss 4.5011, val loss 4.5200\n",
      "step 740: train loss 4.5071, val loss 4.4787\n",
      "step 750: train loss 4.5550, val loss 4.5405\n",
      "step 760: train loss 4.4952, val loss 4.5067\n",
      "step 770: train loss 4.4877, val loss 4.4985\n",
      "step 780: train loss 4.4621, val loss 4.4760\n",
      "step 790: train loss 4.5020, val loss 4.4768\n",
      "step 800: train loss 4.4683, val loss 4.5378\n",
      "step 810: train loss 4.3882, val loss 4.4965\n",
      "step 820: train loss 4.3966, val loss 4.4726\n",
      "step 830: train loss 4.4787, val loss 4.4671\n",
      "step 840: train loss 4.4258, val loss 4.4774\n",
      "step 850: train loss 4.4537, val loss 4.4227\n",
      "step 860: train loss 4.3580, val loss 4.4667\n",
      "step 870: train loss 4.4245, val loss 4.4424\n",
      "step 880: train loss 4.4991, val loss 4.4267\n",
      "step 890: train loss 4.4783, val loss 4.4467\n",
      "step 900: train loss 4.3965, val loss 4.3772\n",
      "step 910: train loss 4.3504, val loss 4.4228\n",
      "step 920: train loss 4.4372, val loss 4.4397\n",
      "step 930: train loss 4.3547, val loss 4.3996\n",
      "step 940: train loss 4.3913, val loss 4.3902\n",
      "step 950: train loss 4.4252, val loss 4.3948\n",
      "step 960: train loss 4.3597, val loss 4.3134\n",
      "step 970: train loss 4.2861, val loss 4.3390\n",
      "step 980: train loss 4.3949, val loss 4.3741\n",
      "step 990: train loss 4.3668, val loss 4.3751\n",
      "step 1000: train loss 4.3398, val loss 4.3240\n",
      "step 1010: train loss 4.2950, val loss 4.3214\n",
      "step 1020: train loss 4.3483, val loss 4.4014\n",
      "step 1030: train loss 4.2744, val loss 4.3443\n",
      "step 1040: train loss 4.2609, val loss 4.3688\n",
      "step 1050: train loss 4.2126, val loss 4.3052\n",
      "step 1060: train loss 4.2883, val loss 4.2839\n",
      "step 1070: train loss 4.2912, val loss 4.2708\n",
      "step 1080: train loss 4.2894, val loss 4.3028\n",
      "step 1090: train loss 4.2522, val loss 4.2268\n",
      "step 1100: train loss 4.2339, val loss 4.2435\n",
      "step 1110: train loss 4.3080, val loss 4.2766\n",
      "step 1120: train loss 4.3010, val loss 4.2654\n",
      "step 1130: train loss 4.2020, val loss 4.2592\n",
      "step 1140: train loss 4.2433, val loss 4.2465\n",
      "step 1150: train loss 4.2208, val loss 4.2082\n",
      "step 1160: train loss 4.2790, val loss 4.2675\n",
      "step 1170: train loss 4.1643, val loss 4.2206\n",
      "step 1180: train loss 4.1917, val loss 4.2190\n",
      "step 1190: train loss 4.2213, val loss 4.1553\n",
      "step 1200: train loss 4.2783, val loss 4.1763\n",
      "step 1210: train loss 4.1850, val loss 4.2437\n",
      "step 1220: train loss 4.1684, val loss 4.1919\n",
      "step 1230: train loss 4.1681, val loss 4.1727\n",
      "step 1240: train loss 4.2514, val loss 4.1484\n",
      "step 1250: train loss 4.2012, val loss 4.1836\n",
      "step 1260: train loss 4.2263, val loss 4.2564\n",
      "step 1270: train loss 4.1373, val loss 4.1660\n",
      "step 1280: train loss 4.1757, val loss 4.2101\n",
      "step 1290: train loss 4.1788, val loss 4.1774\n",
      "step 1300: train loss 4.1799, val loss 4.1083\n",
      "step 1310: train loss 4.1675, val loss 4.1101\n",
      "step 1320: train loss 4.1236, val loss 4.1295\n",
      "step 1330: train loss 4.0897, val loss 4.1326\n",
      "step 1340: train loss 4.1192, val loss 4.0774\n",
      "step 1350: train loss 4.1325, val loss 4.2034\n",
      "step 1360: train loss 4.0576, val loss 4.1566\n",
      "step 1370: train loss 4.0950, val loss 4.1346\n",
      "step 1380: train loss 4.0580, val loss 4.0637\n",
      "step 1390: train loss 4.1061, val loss 4.0947\n",
      "step 1400: train loss 4.0885, val loss 4.1279\n",
      "step 1410: train loss 4.0935, val loss 4.1206\n",
      "step 1420: train loss 4.0572, val loss 4.0710\n",
      "step 1430: train loss 4.0227, val loss 4.0828\n",
      "step 1440: train loss 4.0464, val loss 4.1468\n",
      "step 1450: train loss 4.0292, val loss 3.9862\n",
      "step 1460: train loss 4.0295, val loss 4.0452\n",
      "step 1470: train loss 4.0643, val loss 4.1506\n",
      "step 1480: train loss 4.0333, val loss 4.0326\n",
      "step 1490: train loss 3.9400, val loss 4.0358\n",
      "step 1500: train loss 4.0135, val loss 3.9571\n",
      "step 1510: train loss 4.0465, val loss 4.0028\n",
      "step 1520: train loss 4.0367, val loss 4.0446\n",
      "step 1530: train loss 3.8863, val loss 3.9937\n",
      "step 1540: train loss 4.0533, val loss 4.1114\n",
      "step 1550: train loss 3.9005, val loss 3.9438\n",
      "step 1560: train loss 3.9782, val loss 4.0251\n",
      "step 1570: train loss 4.0671, val loss 3.9638\n",
      "step 1580: train loss 3.9574, val loss 4.0159\n",
      "step 1590: train loss 3.9456, val loss 3.9243\n",
      "step 1600: train loss 3.9342, val loss 3.9938\n",
      "step 1610: train loss 3.9755, val loss 3.9477\n",
      "step 1620: train loss 3.8757, val loss 3.9195\n",
      "step 1630: train loss 3.9097, val loss 3.8946\n",
      "step 1640: train loss 3.8740, val loss 3.9607\n",
      "step 1650: train loss 3.9478, val loss 3.9295\n",
      "step 1660: train loss 3.9410, val loss 3.9533\n",
      "step 1670: train loss 3.9518, val loss 3.9630\n",
      "step 1680: train loss 3.8645, val loss 3.9404\n",
      "step 1690: train loss 3.8613, val loss 3.8989\n",
      "step 1700: train loss 3.8714, val loss 3.9359\n",
      "step 1710: train loss 3.8756, val loss 3.8759\n",
      "step 1720: train loss 3.8648, val loss 3.9496\n",
      "step 1730: train loss 3.9278, val loss 3.9205\n",
      "step 1740: train loss 3.8284, val loss 3.8670\n",
      "step 1750: train loss 3.8830, val loss 3.8796\n",
      "step 1760: train loss 3.9149, val loss 3.9078\n",
      "step 1770: train loss 3.8112, val loss 3.8179\n",
      "step 1780: train loss 3.8665, val loss 3.8767\n",
      "step 1790: train loss 3.8474, val loss 3.8220\n",
      "step 1800: train loss 3.7814, val loss 3.8266\n",
      "step 1810: train loss 3.8459, val loss 3.9047\n",
      "step 1820: train loss 3.8115, val loss 3.8748\n",
      "step 1830: train loss 3.7926, val loss 3.8940\n",
      "step 1840: train loss 3.7604, val loss 3.7878\n",
      "step 1850: train loss 3.8240, val loss 3.9040\n",
      "step 1860: train loss 3.8293, val loss 3.8397\n",
      "step 1870: train loss 3.7804, val loss 3.8780\n",
      "step 1880: train loss 3.7572, val loss 3.8358\n",
      "step 1890: train loss 3.7863, val loss 3.8629\n",
      "step 1900: train loss 3.7600, val loss 3.7818\n",
      "step 1910: train loss 3.7446, val loss 3.8379\n",
      "step 1920: train loss 3.7882, val loss 3.9160\n",
      "step 1930: train loss 3.7326, val loss 3.7435\n",
      "step 1940: train loss 3.7399, val loss 3.7691\n",
      "step 1950: train loss 3.7421, val loss 3.8096\n",
      "step 1960: train loss 3.8106, val loss 3.8212\n",
      "step 1970: train loss 3.7056, val loss 3.8705\n",
      "step 1980: train loss 3.7344, val loss 3.8377\n",
      "step 1990: train loss 3.7849, val loss 3.7547\n",
      "step 2000: train loss 3.7434, val loss 3.8072\n",
      "step 2010: train loss 3.7141, val loss 3.7917\n",
      "step 2020: train loss 3.7775, val loss 3.6908\n",
      "step 2030: train loss 3.7057, val loss 3.7563\n",
      "step 2040: train loss 3.7216, val loss 3.6970\n",
      "step 2050: train loss 3.7018, val loss 3.8091\n",
      "step 2060: train loss 3.6985, val loss 3.7494\n",
      "step 2070: train loss 3.6577, val loss 3.7731\n",
      "step 2080: train loss 3.6361, val loss 3.6810\n",
      "step 2090: train loss 3.6681, val loss 3.7569\n",
      "step 2100: train loss 3.6250, val loss 3.7281\n",
      "step 2110: train loss 3.7067, val loss 3.6660\n",
      "step 2120: train loss 3.6751, val loss 3.7350\n",
      "step 2130: train loss 3.6739, val loss 3.5853\n",
      "step 2140: train loss 3.6314, val loss 3.6285\n",
      "step 2150: train loss 3.6646, val loss 3.7114\n",
      "step 2160: train loss 3.6015, val loss 3.7064\n",
      "step 2170: train loss 3.6381, val loss 3.6934\n",
      "step 2180: train loss 3.6616, val loss 3.5836\n",
      "step 2190: train loss 3.6817, val loss 3.6491\n",
      "step 2200: train loss 3.6516, val loss 3.6964\n",
      "step 2210: train loss 3.6035, val loss 3.7542\n",
      "step 2220: train loss 3.6212, val loss 3.6544\n",
      "step 2230: train loss 3.5549, val loss 3.6931\n",
      "step 2240: train loss 3.5795, val loss 3.6262\n",
      "step 2250: train loss 3.6020, val loss 3.6511\n",
      "step 2260: train loss 3.5762, val loss 3.6746\n",
      "step 2270: train loss 3.6294, val loss 3.6249\n",
      "step 2280: train loss 3.5779, val loss 3.6488\n",
      "step 2290: train loss 3.5800, val loss 3.6452\n",
      "step 2300: train loss 3.6132, val loss 3.6117\n",
      "step 2310: train loss 3.5128, val loss 3.6816\n",
      "step 2320: train loss 3.5702, val loss 3.6360\n",
      "step 2330: train loss 3.5685, val loss 3.6213\n",
      "step 2340: train loss 3.5519, val loss 3.6810\n",
      "step 2350: train loss 3.5240, val loss 3.6451\n",
      "step 2360: train loss 3.5212, val loss 3.6063\n",
      "step 2370: train loss 3.4576, val loss 3.5808\n",
      "step 2380: train loss 3.5384, val loss 3.5604\n",
      "step 2390: train loss 3.5473, val loss 3.5257\n",
      "step 2400: train loss 3.5817, val loss 3.6472\n",
      "step 2410: train loss 3.5680, val loss 3.6740\n",
      "step 2420: train loss 3.5349, val loss 3.5204\n",
      "step 2430: train loss 3.5144, val loss 3.5691\n",
      "step 2440: train loss 3.5366, val loss 3.5960\n",
      "step 2450: train loss 3.4763, val loss 3.5936\n",
      "step 2460: train loss 3.4004, val loss 3.6004\n",
      "step 2470: train loss 3.4560, val loss 3.5507\n",
      "step 2480: train loss 3.5069, val loss 3.5163\n",
      "step 2490: train loss 3.5175, val loss 3.4762\n",
      "step 2500: train loss 3.5483, val loss 3.5182\n",
      "step 2510: train loss 3.4614, val loss 3.5241\n",
      "step 2520: train loss 3.5204, val loss 3.5253\n",
      "step 2530: train loss 3.4387, val loss 3.5460\n",
      "step 2540: train loss 3.4917, val loss 3.5517\n",
      "step 2550: train loss 3.4861, val loss 3.5094\n",
      "step 2560: train loss 3.5034, val loss 3.4773\n",
      "step 2570: train loss 3.4372, val loss 3.5311\n",
      "step 2580: train loss 3.4743, val loss 3.5296\n",
      "step 2590: train loss 3.4071, val loss 3.4816\n",
      "step 2600: train loss 3.5237, val loss 3.5147\n",
      "step 2610: train loss 3.3985, val loss 3.4563\n",
      "step 2620: train loss 3.4610, val loss 3.5243\n",
      "step 2630: train loss 3.4490, val loss 3.5088\n",
      "step 2640: train loss 3.3386, val loss 3.4154\n",
      "step 2650: train loss 3.4768, val loss 3.4856\n",
      "step 2660: train loss 3.4062, val loss 3.4005\n",
      "step 2670: train loss 3.4458, val loss 3.5440\n",
      "step 2680: train loss 3.4547, val loss 3.5644\n",
      "step 2690: train loss 3.4275, val loss 3.4614\n",
      "step 2700: train loss 3.4418, val loss 3.4003\n",
      "step 2710: train loss 3.4510, val loss 3.4455\n",
      "step 2720: train loss 3.4396, val loss 3.5280\n",
      "step 2730: train loss 3.4699, val loss 3.4971\n",
      "step 2740: train loss 3.4727, val loss 3.3890\n",
      "step 2750: train loss 3.3835, val loss 3.4921\n",
      "step 2760: train loss 3.2797, val loss 3.3911\n",
      "step 2770: train loss 3.3963, val loss 3.4317\n",
      "step 2780: train loss 3.4110, val loss 3.4657\n",
      "step 2790: train loss 3.3546, val loss 3.5061\n",
      "step 2800: train loss 3.4601, val loss 3.4439\n",
      "step 2810: train loss 3.3917, val loss 3.3908\n",
      "step 2820: train loss 3.4116, val loss 3.4218\n",
      "step 2830: train loss 3.4261, val loss 3.4205\n",
      "step 2840: train loss 3.4041, val loss 3.4300\n",
      "step 2850: train loss 3.3177, val loss 3.4539\n",
      "step 2860: train loss 3.3287, val loss 3.3760\n",
      "step 2870: train loss 3.3072, val loss 3.4558\n",
      "step 2880: train loss 3.3438, val loss 3.3900\n",
      "step 2890: train loss 3.3776, val loss 3.3445\n",
      "step 2900: train loss 3.2179, val loss 3.3892\n",
      "step 2910: train loss 3.3827, val loss 3.3640\n",
      "step 2920: train loss 3.3634, val loss 3.3337\n",
      "step 2930: train loss 3.3520, val loss 3.3223\n",
      "step 2940: train loss 3.3397, val loss 3.4046\n",
      "step 2950: train loss 3.3796, val loss 3.3676\n",
      "step 2960: train loss 3.2715, val loss 3.4087\n",
      "step 2970: train loss 3.2528, val loss 3.2820\n",
      "step 2980: train loss 3.3079, val loss 3.4038\n",
      "step 2990: train loss 3.3474, val loss 3.3540\n",
      "step 3000: train loss 3.3311, val loss 3.4193\n",
      "step 3010: train loss 3.2534, val loss 3.3580\n",
      "step 3020: train loss 3.2097, val loss 3.2984\n",
      "step 3030: train loss 3.2518, val loss 3.3342\n",
      "step 3040: train loss 3.3229, val loss 3.4271\n",
      "step 3050: train loss 3.3310, val loss 3.3157\n",
      "step 3060: train loss 3.4112, val loss 3.4597\n",
      "step 3070: train loss 3.2947, val loss 3.3358\n",
      "step 3080: train loss 3.2442, val loss 3.3941\n",
      "step 3090: train loss 3.2186, val loss 3.4259\n",
      "step 3100: train loss 3.2387, val loss 3.3005\n",
      "step 3110: train loss 3.2562, val loss 3.2523\n",
      "step 3120: train loss 3.2843, val loss 3.3248\n",
      "step 3130: train loss 3.2662, val loss 3.4166\n",
      "step 3140: train loss 3.2656, val loss 3.4042\n",
      "step 3150: train loss 3.3073, val loss 3.4530\n",
      "step 3160: train loss 3.2931, val loss 3.3936\n",
      "step 3170: train loss 3.2442, val loss 3.3092\n",
      "step 3180: train loss 3.1965, val loss 3.3241\n",
      "step 3190: train loss 3.2106, val loss 3.2868\n",
      "step 3200: train loss 3.2075, val loss 3.2799\n",
      "step 3210: train loss 3.2912, val loss 3.3007\n",
      "step 3220: train loss 3.2215, val loss 3.3132\n",
      "step 3230: train loss 3.2183, val loss 3.3519\n",
      "step 3240: train loss 3.1731, val loss 3.3529\n",
      "step 3250: train loss 3.1925, val loss 3.1538\n",
      "step 3260: train loss 3.1589, val loss 3.3117\n",
      "step 3270: train loss 3.3156, val loss 3.2561\n",
      "step 3280: train loss 3.2166, val loss 3.2571\n",
      "step 3290: train loss 3.1699, val loss 3.3553\n",
      "step 3300: train loss 3.2580, val loss 3.2343\n",
      "step 3310: train loss 3.3001, val loss 3.2302\n",
      "step 3320: train loss 3.1071, val loss 3.1625\n",
      "step 3330: train loss 3.2177, val loss 3.3624\n",
      "step 3340: train loss 3.2186, val loss 3.2832\n",
      "step 3350: train loss 3.1842, val loss 3.2507\n",
      "step 3360: train loss 3.2531, val loss 3.2027\n",
      "step 3370: train loss 3.2580, val loss 3.2760\n",
      "step 3380: train loss 3.1952, val loss 3.3093\n",
      "step 3390: train loss 3.1748, val loss 3.2256\n",
      "step 3400: train loss 3.1877, val loss 3.2516\n",
      "step 3410: train loss 3.0945, val loss 3.2746\n",
      "step 3420: train loss 3.1673, val loss 3.2262\n",
      "step 3430: train loss 3.1497, val loss 3.2616\n",
      "step 3440: train loss 3.0796, val loss 3.2662\n",
      "step 3450: train loss 3.1294, val loss 3.2133\n",
      "step 3460: train loss 3.1375, val loss 3.2734\n",
      "step 3470: train loss 3.2094, val loss 3.2921\n",
      "step 3480: train loss 3.1843, val loss 3.2366\n",
      "step 3490: train loss 3.1395, val loss 3.2487\n",
      "step 3500: train loss 3.0967, val loss 3.2289\n",
      "step 3510: train loss 3.1025, val loss 3.2065\n",
      "step 3520: train loss 3.1236, val loss 3.2908\n",
      "step 3530: train loss 3.0967, val loss 3.1319\n",
      "step 3540: train loss 3.2402, val loss 3.2067\n",
      "step 3550: train loss 3.1472, val loss 3.2924\n",
      "step 3560: train loss 3.2055, val loss 3.2157\n",
      "step 3570: train loss 3.0232, val loss 3.1914\n",
      "step 3580: train loss 3.2019, val loss 3.1338\n",
      "step 3590: train loss 3.1986, val loss 3.0754\n",
      "step 3600: train loss 3.1227, val loss 3.2026\n",
      "step 3610: train loss 3.0944, val loss 3.1031\n",
      "step 3620: train loss 3.0705, val loss 3.2574\n",
      "step 3630: train loss 3.1703, val loss 3.1396\n",
      "step 3640: train loss 3.0267, val loss 3.1727\n",
      "step 3650: train loss 3.1448, val loss 3.1265\n",
      "step 3660: train loss 3.1052, val loss 3.1651\n",
      "step 3670: train loss 3.1268, val loss 3.1469\n",
      "step 3680: train loss 3.1489, val loss 3.1871\n",
      "step 3690: train loss 3.1146, val loss 3.1560\n",
      "step 3700: train loss 3.1165, val loss 3.1507\n",
      "step 3710: train loss 3.1678, val loss 3.1130\n",
      "step 3720: train loss 3.1243, val loss 3.1533\n",
      "step 3730: train loss 3.1181, val loss 3.1437\n",
      "step 3740: train loss 3.1189, val loss 3.1321\n",
      "step 3750: train loss 3.1541, val loss 3.1918\n",
      "step 3760: train loss 3.0451, val loss 3.0723\n",
      "step 3770: train loss 3.0713, val loss 3.0671\n",
      "step 3780: train loss 3.0870, val loss 3.2377\n",
      "step 3790: train loss 3.0487, val loss 3.1956\n",
      "step 3800: train loss 3.0940, val loss 3.1716\n",
      "step 3810: train loss 3.0601, val loss 3.1038\n",
      "step 3820: train loss 3.0531, val loss 3.1202\n",
      "step 3830: train loss 3.1038, val loss 3.0871\n",
      "step 3840: train loss 3.0397, val loss 3.0962\n",
      "step 3850: train loss 2.9473, val loss 3.0497\n",
      "step 3860: train loss 3.0836, val loss 3.1468\n",
      "step 3870: train loss 3.1616, val loss 3.1433\n",
      "step 3880: train loss 3.0745, val loss 3.1057\n",
      "step 3890: train loss 3.0724, val loss 3.0875\n",
      "step 3900: train loss 3.0743, val loss 3.0508\n",
      "step 3910: train loss 3.0763, val loss 3.0367\n",
      "step 3920: train loss 3.1323, val loss 3.0823\n",
      "step 3930: train loss 2.9792, val loss 3.1263\n",
      "step 3940: train loss 3.0531, val loss 3.0508\n",
      "step 3950: train loss 3.0519, val loss 3.1444\n",
      "step 3960: train loss 3.0547, val loss 3.0158\n",
      "step 3970: train loss 3.0440, val loss 3.0564\n",
      "step 3980: train loss 3.0451, val loss 3.1591\n",
      "step 3990: train loss 2.9934, val loss 3.0246\n",
      "step 4000: train loss 3.0192, val loss 3.1514\n",
      "step 4010: train loss 2.9845, val loss 3.1204\n",
      "step 4020: train loss 3.0098, val loss 3.0435\n",
      "step 4030: train loss 3.0501, val loss 3.0540\n",
      "step 4040: train loss 3.0399, val loss 3.0912\n",
      "step 4050: train loss 3.0074, val loss 3.1658\n",
      "step 4060: train loss 2.9869, val loss 3.1203\n",
      "step 4070: train loss 2.9922, val loss 3.1378\n",
      "step 4080: train loss 2.9802, val loss 3.1102\n",
      "step 4090: train loss 3.0287, val loss 3.1203\n",
      "step 4100: train loss 3.0235, val loss 3.0548\n",
      "step 4110: train loss 3.0225, val loss 3.1061\n",
      "step 4120: train loss 3.0089, val loss 3.1433\n",
      "step 4130: train loss 3.0964, val loss 3.1338\n",
      "step 4140: train loss 3.0439, val loss 3.1752\n",
      "step 4150: train loss 2.9567, val loss 3.0870\n",
      "step 4160: train loss 2.9598, val loss 3.1399\n",
      "step 4170: train loss 2.9741, val loss 3.0447\n",
      "step 4180: train loss 2.9977, val loss 3.0033\n",
      "step 4190: train loss 3.0027, val loss 3.0214\n",
      "step 4200: train loss 3.0179, val loss 3.0392\n",
      "step 4210: train loss 2.9432, val loss 3.0286\n",
      "step 4220: train loss 2.9432, val loss 3.0709\n",
      "step 4230: train loss 2.9228, val loss 3.0080\n",
      "step 4240: train loss 2.9296, val loss 3.0049\n",
      "step 4250: train loss 2.9776, val loss 3.0170\n",
      "step 4260: train loss 2.8896, val loss 3.0484\n",
      "step 4270: train loss 2.9584, val loss 3.0647\n",
      "step 4280: train loss 2.9489, val loss 2.8799\n",
      "step 4290: train loss 3.0813, val loss 3.0350\n",
      "step 4300: train loss 3.0254, val loss 2.9322\n",
      "step 4310: train loss 2.9104, val loss 3.0602\n",
      "step 4320: train loss 2.8989, val loss 2.9836\n",
      "step 4330: train loss 3.0648, val loss 3.0222\n",
      "step 4340: train loss 3.0006, val loss 3.0511\n",
      "step 4350: train loss 2.9727, val loss 2.9513\n",
      "step 4360: train loss 2.9163, val loss 3.1443\n",
      "step 4370: train loss 2.8780, val loss 2.9563\n",
      "step 4380: train loss 2.9587, val loss 3.0080\n",
      "step 4390: train loss 2.9451, val loss 3.0645\n",
      "step 4400: train loss 2.9501, val loss 2.9723\n",
      "step 4410: train loss 2.9554, val loss 2.9097\n",
      "step 4420: train loss 2.9567, val loss 3.0287\n",
      "step 4430: train loss 2.8710, val loss 3.0099\n",
      "step 4440: train loss 2.9547, val loss 3.0314\n",
      "step 4450: train loss 2.9242, val loss 2.9878\n",
      "step 4460: train loss 2.9180, val loss 2.9847\n",
      "step 4470: train loss 2.8496, val loss 3.0045\n",
      "step 4480: train loss 2.9682, val loss 2.9942\n",
      "step 4490: train loss 2.9118, val loss 2.9357\n",
      "step 4500: train loss 2.9326, val loss 3.0117\n",
      "step 4510: train loss 2.8867, val loss 2.9417\n",
      "step 4520: train loss 2.9651, val loss 2.9604\n",
      "step 4530: train loss 2.9807, val loss 2.9126\n",
      "step 4540: train loss 2.8688, val loss 2.9372\n",
      "step 4550: train loss 2.9742, val loss 2.9408\n",
      "step 4560: train loss 2.9519, val loss 2.9965\n",
      "step 4570: train loss 2.9141, val loss 2.9680\n",
      "step 4580: train loss 2.8832, val loss 2.8878\n",
      "step 4590: train loss 2.9074, val loss 2.9186\n",
      "step 4600: train loss 2.9233, val loss 3.0178\n",
      "step 4610: train loss 2.9754, val loss 2.9559\n",
      "step 4620: train loss 2.8957, val loss 2.9144\n",
      "step 4630: train loss 2.8567, val loss 2.8981\n",
      "step 4640: train loss 2.8987, val loss 2.9295\n",
      "step 4650: train loss 2.8866, val loss 3.0580\n",
      "step 4660: train loss 2.9386, val loss 3.1167\n",
      "step 4670: train loss 2.9139, val loss 3.0172\n",
      "step 4680: train loss 2.9050, val loss 2.9981\n",
      "step 4690: train loss 2.8688, val loss 2.8509\n",
      "step 4700: train loss 2.9441, val loss 2.9605\n",
      "step 4710: train loss 2.9230, val loss 2.9191\n",
      "step 4720: train loss 2.9000, val loss 2.9494\n",
      "step 4730: train loss 2.8771, val loss 2.8835\n",
      "step 4740: train loss 2.8468, val loss 3.0094\n",
      "step 4750: train loss 2.8121, val loss 2.8978\n",
      "step 4760: train loss 2.8982, val loss 2.9904\n",
      "step 4770: train loss 2.8709, val loss 2.9337\n",
      "step 4780: train loss 2.8285, val loss 2.8734\n",
      "step 4790: train loss 2.9049, val loss 2.9479\n",
      "step 4800: train loss 2.8762, val loss 2.9568\n",
      "step 4810: train loss 2.8738, val loss 2.9382\n",
      "step 4820: train loss 2.8624, val loss 2.9320\n",
      "step 4830: train loss 2.8726, val loss 2.9798\n",
      "step 4840: train loss 2.8644, val loss 2.8341\n",
      "step 4850: train loss 2.7742, val loss 2.9120\n",
      "step 4860: train loss 2.9386, val loss 2.9512\n",
      "step 4870: train loss 2.9339, val loss 3.0911\n",
      "step 4880: train loss 2.8422, val loss 2.9552\n",
      "step 4890: train loss 2.8906, val loss 2.9266\n",
      "step 4900: train loss 2.8700, val loss 2.9088\n",
      "step 4910: train loss 2.8906, val loss 2.8443\n",
      "step 4920: train loss 2.8792, val loss 2.9254\n",
      "step 4930: train loss 2.8935, val loss 2.8658\n",
      "step 4940: train loss 2.8638, val loss 2.9468\n",
      "step 4950: train loss 2.9096, val loss 2.8599\n",
      "step 4960: train loss 2.8469, val loss 2.9902\n",
      "step 4970: train loss 2.8772, val loss 2.9777\n",
      "step 4980: train loss 2.8400, val loss 2.8733\n",
      "step 4990: train loss 2.8472, val loss 3.0009\n",
      "step 5000: train loss 2.8038, val loss 2.9157\n",
      "step 5010: train loss 2.8183, val loss 2.9146\n",
      "step 5020: train loss 2.8584, val loss 2.8923\n",
      "step 5030: train loss 2.7971, val loss 2.8653\n",
      "step 5040: train loss 2.8937, val loss 2.9595\n",
      "step 5050: train loss 2.8781, val loss 2.9600\n",
      "step 5060: train loss 2.8900, val loss 2.9263\n",
      "step 5070: train loss 2.8510, val loss 2.9070\n",
      "step 5080: train loss 2.7748, val loss 2.9203\n",
      "step 5090: train loss 2.8496, val loss 3.0156\n",
      "step 5100: train loss 2.8515, val loss 2.8146\n",
      "step 5110: train loss 2.7910, val loss 2.9462\n",
      "step 5120: train loss 2.8079, val loss 2.8043\n",
      "step 5130: train loss 2.7929, val loss 2.9078\n",
      "step 5140: train loss 2.8311, val loss 2.8280\n",
      "step 5150: train loss 2.8047, val loss 2.7945\n",
      "step 5160: train loss 2.8743, val loss 2.9485\n",
      "step 5170: train loss 2.8218, val loss 2.8624\n",
      "step 5180: train loss 2.8073, val loss 2.8747\n",
      "step 5190: train loss 2.8193, val loss 2.8960\n",
      "step 5200: train loss 2.8385, val loss 2.9172\n",
      "step 5210: train loss 2.7696, val loss 2.8463\n",
      "step 5220: train loss 2.8220, val loss 2.9220\n",
      "step 5230: train loss 2.8653, val loss 2.7868\n",
      "step 5240: train loss 2.7674, val loss 2.8164\n",
      "step 5250: train loss 2.8636, val loss 2.8610\n",
      "step 5260: train loss 2.8748, val loss 2.7674\n",
      "step 5270: train loss 2.8311, val loss 2.9181\n",
      "step 5280: train loss 2.7729, val loss 2.9130\n",
      "step 5290: train loss 2.7917, val loss 2.8346\n",
      "step 5300: train loss 2.7314, val loss 2.9050\n",
      "step 5310: train loss 2.8326, val loss 2.8334\n",
      "step 5320: train loss 2.8163, val loss 2.8792\n",
      "step 5330: train loss 2.7710, val loss 2.8473\n",
      "step 5340: train loss 2.8062, val loss 2.8638\n",
      "step 5350: train loss 2.7583, val loss 2.9325\n",
      "step 5360: train loss 2.7678, val loss 2.9516\n",
      "step 5370: train loss 2.7739, val loss 2.8254\n",
      "step 5380: train loss 2.7654, val loss 2.8117\n",
      "step 5390: train loss 2.7422, val loss 2.8115\n",
      "step 5400: train loss 2.7251, val loss 2.7813\n",
      "step 5410: train loss 2.7972, val loss 2.8727\n",
      "step 5420: train loss 2.8166, val loss 2.8652\n",
      "step 5430: train loss 2.8416, val loss 2.8168\n",
      "step 5440: train loss 2.8996, val loss 2.9096\n",
      "step 5450: train loss 2.7254, val loss 2.8402\n",
      "step 5460: train loss 2.8301, val loss 2.9384\n",
      "step 5470: train loss 2.7469, val loss 2.8554\n",
      "step 5480: train loss 2.8078, val loss 2.9156\n",
      "step 5490: train loss 2.7150, val loss 2.8659\n",
      "step 5500: train loss 2.7586, val loss 2.8549\n",
      "step 5510: train loss 2.7394, val loss 2.8097\n",
      "step 5520: train loss 2.7263, val loss 2.9409\n",
      "step 5530: train loss 2.7637, val loss 2.8700\n",
      "step 5540: train loss 2.7791, val loss 2.8099\n",
      "step 5550: train loss 2.6919, val loss 2.8011\n",
      "step 5560: train loss 2.7170, val loss 2.9594\n",
      "step 5570: train loss 2.7221, val loss 2.8477\n",
      "step 5580: train loss 2.7787, val loss 2.7814\n",
      "step 5590: train loss 2.6934, val loss 2.9657\n",
      "step 5600: train loss 2.7267, val loss 2.8968\n",
      "step 5610: train loss 2.8055, val loss 2.8343\n",
      "step 5620: train loss 2.7481, val loss 3.0663\n",
      "step 5630: train loss 2.7589, val loss 2.8273\n",
      "step 5640: train loss 2.6196, val loss 2.8270\n",
      "step 5650: train loss 2.7336, val loss 2.9221\n",
      "step 5660: train loss 2.7779, val loss 2.9174\n",
      "step 5670: train loss 2.7089, val loss 2.8687\n",
      "step 5680: train loss 2.7609, val loss 2.8541\n",
      "step 5690: train loss 2.7267, val loss 2.8251\n",
      "step 5700: train loss 2.7069, val loss 2.8180\n",
      "step 5710: train loss 2.7406, val loss 2.7689\n",
      "step 5720: train loss 2.7374, val loss 2.8787\n",
      "step 5730: train loss 2.7096, val loss 2.8794\n",
      "step 5740: train loss 2.7719, val loss 2.7942\n",
      "step 5750: train loss 2.8278, val loss 2.7908\n",
      "step 5760: train loss 2.7681, val loss 2.8019\n",
      "step 5770: train loss 2.8217, val loss 2.8232\n",
      "step 5780: train loss 2.7441, val loss 2.8578\n",
      "step 5790: train loss 2.8415, val loss 2.8402\n",
      "step 5800: train loss 2.7101, val loss 2.8349\n",
      "step 5810: train loss 2.7679, val loss 2.8799\n",
      "step 5820: train loss 2.7453, val loss 2.8370\n",
      "step 5830: train loss 2.7250, val loss 2.8892\n",
      "step 5840: train loss 2.7271, val loss 2.7757\n",
      "step 5850: train loss 2.7827, val loss 2.8290\n",
      "step 5860: train loss 2.7032, val loss 2.8370\n",
      "step 5870: train loss 2.7583, val loss 2.9027\n",
      "step 5880: train loss 2.8189, val loss 2.7807\n",
      "step 5890: train loss 2.7383, val loss 2.7183\n",
      "step 5900: train loss 2.7245, val loss 2.9298\n",
      "step 5910: train loss 2.6718, val loss 2.7625\n",
      "step 5920: train loss 2.6835, val loss 2.7324\n",
      "step 5930: train loss 2.7353, val loss 2.8355\n",
      "step 5940: train loss 2.8041, val loss 2.8061\n",
      "step 5950: train loss 2.6954, val loss 2.8422\n",
      "step 5960: train loss 2.6628, val loss 2.8050\n",
      "step 5970: train loss 2.7445, val loss 2.7072\n",
      "step 5980: train loss 2.6631, val loss 2.7477\n",
      "step 5990: train loss 2.7538, val loss 2.7861\n",
      "step 6000: train loss 2.7752, val loss 2.7973\n",
      "step 6010: train loss 2.6881, val loss 2.7253\n",
      "step 6020: train loss 2.7553, val loss 2.8675\n",
      "step 6030: train loss 2.7162, val loss 2.6601\n",
      "step 6040: train loss 2.6441, val loss 2.8678\n",
      "step 6050: train loss 2.7074, val loss 2.7732\n",
      "step 6060: train loss 2.6902, val loss 2.8014\n",
      "step 6070: train loss 2.7910, val loss 2.6842\n",
      "step 6080: train loss 2.7711, val loss 2.8207\n",
      "step 6090: train loss 2.7103, val loss 2.8357\n",
      "step 6100: train loss 2.6157, val loss 2.7720\n",
      "step 6110: train loss 2.7430, val loss 2.7344\n",
      "step 6120: train loss 2.7994, val loss 2.7873\n",
      "step 6130: train loss 2.7173, val loss 2.7539\n",
      "step 6140: train loss 2.7462, val loss 2.8797\n",
      "step 6150: train loss 2.7694, val loss 2.8486\n",
      "step 6160: train loss 2.7219, val loss 2.7075\n",
      "step 6170: train loss 2.7525, val loss 2.7361\n",
      "step 6180: train loss 2.6942, val loss 2.8107\n",
      "step 6190: train loss 2.6678, val loss 2.7751\n",
      "step 6200: train loss 2.7048, val loss 2.7969\n",
      "step 6210: train loss 2.7690, val loss 2.8327\n",
      "step 6220: train loss 2.7116, val loss 2.7206\n",
      "step 6230: train loss 2.6833, val loss 2.7444\n",
      "step 6240: train loss 2.7452, val loss 2.7478\n",
      "step 6250: train loss 2.6669, val loss 2.8070\n",
      "step 6260: train loss 2.6644, val loss 2.7869\n",
      "step 6270: train loss 2.6914, val loss 2.7771\n",
      "step 6280: train loss 2.6083, val loss 2.8342\n",
      "step 6290: train loss 2.7039, val loss 2.7763\n",
      "step 6300: train loss 2.6660, val loss 2.6966\n",
      "step 6310: train loss 2.6926, val loss 2.8304\n",
      "step 6320: train loss 2.6670, val loss 2.6997\n",
      "step 6330: train loss 2.6692, val loss 2.8226\n",
      "step 6340: train loss 2.6457, val loss 2.6191\n",
      "step 6350: train loss 2.6869, val loss 2.7442\n",
      "step 6360: train loss 2.6821, val loss 2.8006\n",
      "step 6370: train loss 2.7319, val loss 2.8121\n",
      "step 6380: train loss 2.5800, val loss 2.7552\n",
      "step 6390: train loss 2.6480, val loss 2.7512\n",
      "step 6400: train loss 2.6629, val loss 2.8280\n",
      "step 6410: train loss 2.6509, val loss 2.9344\n",
      "step 6420: train loss 2.7002, val loss 2.7326\n",
      "step 6430: train loss 2.6668, val loss 2.7861\n",
      "step 6440: train loss 2.6457, val loss 2.7586\n",
      "step 6450: train loss 2.6948, val loss 2.7536\n",
      "step 6460: train loss 2.7351, val loss 2.6660\n",
      "step 6470: train loss 2.6851, val loss 2.7554\n",
      "step 6480: train loss 2.6621, val loss 2.7438\n",
      "step 6490: train loss 2.6612, val loss 2.7610\n",
      "step 6500: train loss 2.7303, val loss 2.6875\n",
      "step 6510: train loss 2.5924, val loss 2.7899\n",
      "step 6520: train loss 2.6995, val loss 2.8830\n",
      "step 6530: train loss 2.6679, val loss 2.7570\n",
      "step 6540: train loss 2.5916, val loss 2.7740\n",
      "step 6550: train loss 2.6668, val loss 2.7741\n",
      "step 6560: train loss 2.6606, val loss 2.7557\n",
      "step 6570: train loss 2.5868, val loss 2.6774\n",
      "step 6580: train loss 2.7130, val loss 2.7022\n",
      "step 6590: train loss 2.6016, val loss 2.6465\n",
      "step 6600: train loss 2.5394, val loss 2.7380\n",
      "step 6610: train loss 2.7313, val loss 2.7076\n",
      "step 6620: train loss 2.6450, val loss 2.6821\n",
      "step 6630: train loss 2.6998, val loss 2.7690\n",
      "step 6640: train loss 2.7028, val loss 2.7489\n",
      "step 6650: train loss 2.6465, val loss 2.7283\n",
      "step 6660: train loss 2.6959, val loss 2.6759\n",
      "step 6670: train loss 2.5836, val loss 2.7016\n",
      "step 6680: train loss 2.6366, val loss 2.7011\n",
      "step 6690: train loss 2.6074, val loss 2.7342\n",
      "step 6700: train loss 2.6380, val loss 2.7076\n",
      "step 6710: train loss 2.6874, val loss 2.5795\n",
      "step 6720: train loss 2.5692, val loss 2.7402\n",
      "step 6730: train loss 2.6890, val loss 2.6873\n",
      "step 6740: train loss 2.5960, val loss 2.6629\n",
      "step 6750: train loss 2.6653, val loss 2.6684\n",
      "step 6760: train loss 2.7199, val loss 2.6593\n",
      "step 6770: train loss 2.6131, val loss 2.7005\n",
      "step 6780: train loss 2.6849, val loss 2.7454\n",
      "step 6790: train loss 2.6452, val loss 2.7180\n",
      "step 6800: train loss 2.6794, val loss 2.7111\n",
      "step 6810: train loss 2.6603, val loss 2.7668\n",
      "step 6820: train loss 2.5911, val loss 2.7156\n",
      "step 6830: train loss 2.5964, val loss 2.7920\n",
      "step 6840: train loss 2.5679, val loss 2.7060\n",
      "step 6850: train loss 2.6232, val loss 2.5992\n",
      "step 6860: train loss 2.5408, val loss 2.7473\n",
      "step 6870: train loss 2.6032, val loss 2.6520\n",
      "step 6880: train loss 2.6540, val loss 2.7925\n",
      "step 6890: train loss 2.6035, val loss 2.7571\n",
      "step 6900: train loss 2.6727, val loss 2.6680\n",
      "step 6910: train loss 2.6254, val loss 2.7948\n",
      "step 6920: train loss 2.5596, val loss 2.6731\n",
      "step 6930: train loss 2.6112, val loss 2.7737\n",
      "step 6940: train loss 2.6659, val loss 2.6855\n",
      "step 6950: train loss 2.7090, val loss 2.6264\n",
      "step 6960: train loss 2.6598, val loss 2.7113\n",
      "step 6970: train loss 2.6332, val loss 2.7272\n",
      "step 6980: train loss 2.6081, val loss 2.7809\n",
      "step 6990: train loss 2.7476, val loss 2.7400\n",
      "step 7000: train loss 2.5806, val loss 2.7654\n",
      "step 7010: train loss 2.6190, val loss 2.6900\n",
      "step 7020: train loss 2.6244, val loss 2.7395\n",
      "step 7030: train loss 2.6332, val loss 2.7619\n",
      "step 7040: train loss 2.5651, val loss 2.7040\n",
      "step 7050: train loss 2.6310, val loss 2.7861\n",
      "step 7060: train loss 2.5560, val loss 2.6841\n",
      "step 7070: train loss 2.5531, val loss 2.6019\n",
      "step 7080: train loss 2.6735, val loss 2.6904\n",
      "step 7090: train loss 2.6246, val loss 2.6474\n",
      "step 7100: train loss 2.5988, val loss 2.6402\n",
      "step 7110: train loss 2.5684, val loss 2.7007\n",
      "step 7120: train loss 2.5925, val loss 2.6676\n",
      "step 7130: train loss 2.6025, val loss 2.7072\n",
      "step 7140: train loss 2.5900, val loss 2.6955\n",
      "step 7150: train loss 2.5494, val loss 2.6503\n",
      "step 7160: train loss 2.5602, val loss 2.7967\n",
      "step 7170: train loss 2.6087, val loss 2.7479\n",
      "step 7180: train loss 2.5758, val loss 2.6965\n",
      "step 7190: train loss 2.6812, val loss 2.6103\n",
      "step 7200: train loss 2.6652, val loss 2.7116\n",
      "step 7210: train loss 2.5497, val loss 2.6675\n",
      "step 7220: train loss 2.6074, val loss 2.6937\n",
      "step 7230: train loss 2.5568, val loss 2.7105\n",
      "step 7240: train loss 2.6387, val loss 2.8231\n",
      "step 7250: train loss 2.5921, val loss 2.7266\n",
      "step 7260: train loss 2.6180, val loss 2.6879\n",
      "step 7270: train loss 2.6638, val loss 2.7511\n",
      "step 7280: train loss 2.5897, val loss 2.6660\n",
      "step 7290: train loss 2.6299, val loss 2.6675\n",
      "step 7300: train loss 2.6663, val loss 2.5162\n",
      "step 7310: train loss 2.5457, val loss 2.6126\n",
      "step 7320: train loss 2.5417, val loss 2.6309\n",
      "step 7330: train loss 2.6124, val loss 2.6242\n",
      "step 7340: train loss 2.6163, val loss 2.7354\n",
      "step 7350: train loss 2.5881, val loss 2.8165\n",
      "step 7360: train loss 2.5730, val loss 2.7795\n",
      "step 7370: train loss 2.6006, val loss 2.6635\n",
      "step 7380: train loss 2.5513, val loss 2.6302\n",
      "step 7390: train loss 2.6732, val loss 2.7476\n",
      "step 7400: train loss 2.6413, val loss 2.7238\n",
      "step 7410: train loss 2.6046, val loss 2.6421\n",
      "step 7420: train loss 2.5354, val loss 2.7183\n",
      "step 7430: train loss 2.6291, val loss 2.6602\n",
      "step 7440: train loss 2.5601, val loss 2.7188\n",
      "step 7450: train loss 2.6309, val loss 2.7179\n",
      "step 7460: train loss 2.7331, val loss 2.6543\n",
      "step 7470: train loss 2.6007, val loss 2.7052\n",
      "step 7480: train loss 2.5944, val loss 2.8292\n",
      "step 7490: train loss 2.5831, val loss 2.6487\n",
      "step 7500: train loss 2.6128, val loss 2.6471\n",
      "step 7510: train loss 2.5156, val loss 2.6531\n",
      "step 7520: train loss 2.6246, val loss 2.6810\n",
      "step 7530: train loss 2.5569, val loss 2.6162\n",
      "step 7540: train loss 2.5518, val loss 2.6709\n",
      "step 7550: train loss 2.5083, val loss 2.7242\n",
      "step 7560: train loss 2.5760, val loss 2.7079\n",
      "step 7570: train loss 2.5853, val loss 2.7324\n",
      "step 7580: train loss 2.6214, val loss 2.5891\n",
      "step 7590: train loss 2.5490, val loss 2.6405\n",
      "step 7600: train loss 2.6214, val loss 2.5918\n",
      "step 7610: train loss 2.5469, val loss 2.6664\n",
      "step 7620: train loss 2.5438, val loss 2.6454\n",
      "step 7630: train loss 2.5916, val loss 2.7072\n",
      "step 7640: train loss 2.6723, val loss 2.5924\n",
      "step 7650: train loss 2.5084, val loss 2.6024\n",
      "step 7660: train loss 2.5863, val loss 2.6673\n",
      "step 7670: train loss 2.5867, val loss 2.6292\n",
      "step 7680: train loss 2.5843, val loss 2.7318\n",
      "step 7690: train loss 2.6807, val loss 2.7575\n",
      "step 7700: train loss 2.5974, val loss 2.6328\n",
      "step 7710: train loss 2.5658, val loss 2.6627\n",
      "step 7720: train loss 2.5625, val loss 2.6679\n",
      "step 7730: train loss 2.6204, val loss 2.7083\n",
      "step 7740: train loss 2.5644, val loss 2.6790\n",
      "step 7750: train loss 2.5493, val loss 2.5538\n",
      "step 7760: train loss 2.6619, val loss 2.6579\n",
      "step 7770: train loss 2.5669, val loss 2.7289\n",
      "step 7780: train loss 2.6191, val loss 2.6413\n",
      "step 7790: train loss 2.5002, val loss 2.6961\n",
      "step 7800: train loss 2.5532, val loss 2.6941\n",
      "step 7810: train loss 2.5437, val loss 2.7800\n",
      "step 7820: train loss 2.5725, val loss 2.5292\n",
      "step 7830: train loss 2.5819, val loss 2.7251\n",
      "step 7840: train loss 2.6061, val loss 2.6203\n",
      "step 7850: train loss 2.6441, val loss 2.6912\n",
      "step 7860: train loss 2.6159, val loss 2.6731\n",
      "step 7870: train loss 2.5871, val loss 2.6815\n",
      "step 7880: train loss 2.5809, val loss 2.6743\n",
      "step 7890: train loss 2.6161, val loss 2.6572\n",
      "step 7900: train loss 2.5696, val loss 2.5850\n",
      "step 7910: train loss 2.5864, val loss 2.6679\n",
      "step 7920: train loss 2.6316, val loss 2.5963\n",
      "step 7930: train loss 2.5632, val loss 2.7330\n",
      "step 7940: train loss 2.5219, val loss 2.5828\n",
      "step 7950: train loss 2.5385, val loss 2.6860\n",
      "step 7960: train loss 2.5602, val loss 2.5907\n",
      "step 7970: train loss 2.5446, val loss 2.5824\n",
      "step 7980: train loss 2.6462, val loss 2.6098\n",
      "step 7990: train loss 2.5068, val loss 2.6657\n",
      "step 8000: train loss 2.5817, val loss 2.6070\n",
      "step 8010: train loss 2.5207, val loss 2.6472\n",
      "step 8020: train loss 2.5416, val loss 2.5946\n",
      "step 8030: train loss 2.5914, val loss 2.6793\n",
      "step 8040: train loss 2.5896, val loss 2.6002\n",
      "step 8050: train loss 2.5725, val loss 2.5175\n",
      "step 8060: train loss 2.4406, val loss 2.7064\n",
      "step 8070: train loss 2.4976, val loss 2.7062\n",
      "step 8080: train loss 2.5615, val loss 2.6786\n",
      "step 8090: train loss 2.6120, val loss 2.7107\n",
      "step 8100: train loss 2.5107, val loss 2.5835\n",
      "step 8110: train loss 2.5167, val loss 2.4888\n",
      "step 8120: train loss 2.5737, val loss 2.6099\n",
      "step 8130: train loss 2.4652, val loss 2.6272\n",
      "step 8140: train loss 2.5971, val loss 2.6746\n",
      "step 8150: train loss 2.4814, val loss 2.6230\n",
      "step 8160: train loss 2.5590, val loss 2.6931\n",
      "step 8170: train loss 2.6104, val loss 2.5790\n",
      "step 8180: train loss 2.5799, val loss 2.5661\n",
      "step 8190: train loss 2.5955, val loss 2.6491\n",
      "step 8200: train loss 2.5050, val loss 2.6567\n",
      "step 8210: train loss 2.5474, val loss 2.6794\n",
      "step 8220: train loss 2.5555, val loss 2.6379\n",
      "step 8230: train loss 2.6164, val loss 2.6139\n",
      "step 8240: train loss 2.5855, val loss 2.6092\n",
      "step 8250: train loss 2.5457, val loss 2.5605\n",
      "step 8260: train loss 2.5683, val loss 2.6859\n",
      "step 8270: train loss 2.6081, val loss 2.6008\n",
      "step 8280: train loss 2.5465, val loss 2.6562\n",
      "step 8290: train loss 2.5665, val loss 2.6658\n",
      "step 8300: train loss 2.6198, val loss 2.6349\n",
      "step 8310: train loss 2.5514, val loss 2.6749\n",
      "step 8320: train loss 2.4628, val loss 2.6831\n",
      "step 8330: train loss 2.4731, val loss 2.6342\n",
      "step 8340: train loss 2.5713, val loss 2.6325\n",
      "step 8350: train loss 2.6544, val loss 2.6904\n",
      "step 8360: train loss 2.4534, val loss 2.6540\n",
      "step 8370: train loss 2.5083, val loss 2.5211\n",
      "step 8380: train loss 2.6121, val loss 2.6024\n",
      "step 8390: train loss 2.5386, val loss 2.6207\n",
      "step 8400: train loss 2.5607, val loss 2.5844\n",
      "step 8410: train loss 2.5698, val loss 2.6520\n",
      "step 8420: train loss 2.4957, val loss 2.5735\n",
      "step 8430: train loss 2.6384, val loss 2.6367\n",
      "step 8440: train loss 2.5596, val loss 2.6284\n",
      "step 8450: train loss 2.5120, val loss 2.5240\n",
      "step 8460: train loss 2.4940, val loss 2.5895\n",
      "step 8470: train loss 2.5117, val loss 2.5969\n",
      "step 8480: train loss 2.5299, val loss 2.4638\n",
      "step 8490: train loss 2.5041, val loss 2.5770\n",
      "step 8500: train loss 2.4675, val loss 2.5547\n",
      "step 8510: train loss 2.5814, val loss 2.6310\n",
      "step 8520: train loss 2.4815, val loss 2.6293\n",
      "step 8530: train loss 2.4905, val loss 2.5177\n",
      "step 8540: train loss 2.5577, val loss 2.6381\n",
      "step 8550: train loss 2.5651, val loss 2.5736\n",
      "step 8560: train loss 2.4705, val loss 2.6506\n",
      "step 8570: train loss 2.5219, val loss 2.6801\n",
      "step 8580: train loss 2.6179, val loss 2.5786\n",
      "step 8590: train loss 2.5690, val loss 2.6602\n",
      "step 8600: train loss 2.4953, val loss 2.6823\n",
      "step 8610: train loss 2.5748, val loss 2.6230\n",
      "step 8620: train loss 2.5939, val loss 2.5300\n",
      "step 8630: train loss 2.4894, val loss 2.6040\n",
      "step 8640: train loss 2.6162, val loss 2.5686\n",
      "step 8650: train loss 2.5670, val loss 2.6687\n",
      "step 8660: train loss 2.5956, val loss 2.6109\n",
      "step 8670: train loss 2.5157, val loss 2.5711\n",
      "step 8680: train loss 2.4983, val loss 2.5224\n",
      "step 8690: train loss 2.4867, val loss 2.5505\n",
      "step 8700: train loss 2.5107, val loss 2.4842\n",
      "step 8710: train loss 2.5446, val loss 2.5746\n",
      "step 8720: train loss 2.5217, val loss 2.5914\n",
      "step 8730: train loss 2.4778, val loss 2.6252\n",
      "step 8740: train loss 2.5541, val loss 2.6451\n",
      "step 8750: train loss 2.5777, val loss 2.5804\n",
      "step 8760: train loss 2.6108, val loss 2.6243\n",
      "step 8770: train loss 2.5935, val loss 2.6994\n",
      "step 8780: train loss 2.5609, val loss 2.5583\n",
      "step 8790: train loss 2.5675, val loss 2.5316\n",
      "step 8800: train loss 2.5167, val loss 2.6329\n",
      "step 8810: train loss 2.5002, val loss 2.7158\n",
      "step 8820: train loss 2.5634, val loss 2.5699\n",
      "step 8830: train loss 2.4560, val loss 2.5722\n",
      "step 8840: train loss 2.5654, val loss 2.6620\n",
      "step 8850: train loss 2.5360, val loss 2.6421\n",
      "step 8860: train loss 2.5309, val loss 2.6686\n",
      "step 8870: train loss 2.6462, val loss 2.5822\n",
      "step 8880: train loss 2.5195, val loss 2.7147\n",
      "step 8890: train loss 2.5760, val loss 2.5205\n",
      "step 8900: train loss 2.5205, val loss 2.6544\n",
      "step 8910: train loss 2.5676, val loss 2.6105\n",
      "step 8920: train loss 2.5489, val loss 2.5753\n",
      "step 8930: train loss 2.5583, val loss 2.5729\n",
      "step 8940: train loss 2.5977, val loss 2.6568\n",
      "step 8950: train loss 2.5551, val loss 2.6209\n",
      "step 8960: train loss 2.5093, val loss 2.5924\n",
      "step 8970: train loss 2.5309, val loss 2.6541\n",
      "step 8980: train loss 2.5725, val loss 2.6238\n",
      "step 8990: train loss 2.5233, val loss 2.6211\n",
      "step 9000: train loss 2.4446, val loss 2.5805\n",
      "step 9010: train loss 2.5372, val loss 2.5883\n",
      "step 9020: train loss 2.4992, val loss 2.6028\n",
      "step 9030: train loss 2.4305, val loss 2.5976\n",
      "step 9040: train loss 2.5107, val loss 2.5484\n",
      "step 9050: train loss 2.4943, val loss 2.6451\n",
      "step 9060: train loss 2.5856, val loss 2.6793\n",
      "step 9070: train loss 2.5514, val loss 2.5463\n",
      "step 9080: train loss 2.5592, val loss 2.5370\n",
      "step 9090: train loss 2.4585, val loss 2.6388\n",
      "step 9100: train loss 2.5524, val loss 2.6362\n",
      "step 9110: train loss 2.5305, val loss 2.5760\n",
      "step 9120: train loss 2.5236, val loss 2.5694\n",
      "step 9130: train loss 2.5510, val loss 2.6396\n",
      "step 9140: train loss 2.4880, val loss 2.5098\n",
      "step 9150: train loss 2.4856, val loss 2.5900\n",
      "step 9160: train loss 2.5084, val loss 2.6594\n",
      "step 9170: train loss 2.5108, val loss 2.5894\n",
      "step 9180: train loss 2.4273, val loss 2.5275\n",
      "step 9190: train loss 2.5802, val loss 2.6316\n",
      "step 9200: train loss 2.5452, val loss 2.6007\n",
      "step 9210: train loss 2.6563, val loss 2.5759\n",
      "step 9220: train loss 2.5522, val loss 2.4960\n",
      "step 9230: train loss 2.4875, val loss 2.4568\n",
      "step 9240: train loss 2.5207, val loss 2.5383\n",
      "step 9250: train loss 2.5470, val loss 2.6584\n",
      "step 9260: train loss 2.4168, val loss 2.5675\n",
      "step 9270: train loss 2.5406, val loss 2.4826\n",
      "step 9280: train loss 2.5723, val loss 2.5132\n",
      "step 9290: train loss 2.5991, val loss 2.5086\n",
      "step 9300: train loss 2.5138, val loss 2.6861\n",
      "step 9310: train loss 2.4973, val loss 2.4825\n",
      "step 9320: train loss 2.5381, val loss 2.4743\n",
      "step 9330: train loss 2.4377, val loss 2.4926\n",
      "step 9340: train loss 2.4610, val loss 2.5979\n",
      "step 9350: train loss 2.5173, val loss 2.7309\n",
      "step 9360: train loss 2.5115, val loss 2.5003\n",
      "step 9370: train loss 2.5022, val loss 2.7331\n",
      "step 9380: train loss 2.4931, val loss 2.5753\n",
      "step 9390: train loss 2.5007, val loss 2.4753\n",
      "step 9400: train loss 2.4664, val loss 2.7255\n",
      "step 9410: train loss 2.4785, val loss 2.5529\n",
      "step 9420: train loss 2.6114, val loss 2.5603\n",
      "step 9430: train loss 2.4906, val loss 2.6156\n",
      "step 9440: train loss 2.5071, val loss 2.6091\n",
      "step 9450: train loss 2.4377, val loss 2.5108\n",
      "step 9460: train loss 2.5385, val loss 2.6545\n",
      "step 9470: train loss 2.5271, val loss 2.6139\n",
      "step 9480: train loss 2.4401, val loss 2.5599\n",
      "step 9490: train loss 2.4915, val loss 2.4957\n",
      "step 9500: train loss 2.5172, val loss 2.5195\n",
      "step 9510: train loss 2.4925, val loss 2.4851\n",
      "step 9520: train loss 2.4856, val loss 2.5423\n",
      "step 9530: train loss 2.5653, val loss 2.4985\n",
      "step 9540: train loss 2.4645, val loss 2.6837\n",
      "step 9550: train loss 2.4886, val loss 2.5867\n",
      "step 9560: train loss 2.5112, val loss 2.5315\n",
      "step 9570: train loss 2.4799, val loss 2.5828\n",
      "step 9580: train loss 2.4799, val loss 2.5965\n",
      "step 9590: train loss 2.4569, val loss 2.5568\n",
      "step 9600: train loss 2.5324, val loss 2.5459\n",
      "step 9610: train loss 2.6340, val loss 2.6243\n",
      "step 9620: train loss 2.5253, val loss 2.6691\n",
      "step 9630: train loss 2.5539, val loss 2.6932\n",
      "step 9640: train loss 2.5555, val loss 2.4340\n",
      "step 9650: train loss 2.5558, val loss 2.5507\n",
      "step 9660: train loss 2.5088, val loss 2.6595\n",
      "step 9670: train loss 2.5502, val loss 2.5309\n",
      "step 9680: train loss 2.4395, val loss 2.5708\n",
      "step 9690: train loss 2.4913, val loss 2.5031\n",
      "step 9700: train loss 2.4904, val loss 2.5163\n",
      "step 9710: train loss 2.4804, val loss 2.6055\n",
      "step 9720: train loss 2.3857, val loss 2.4757\n",
      "step 9730: train loss 2.5572, val loss 2.6399\n",
      "step 9740: train loss 2.5153, val loss 2.5706\n",
      "step 9750: train loss 2.4664, val loss 2.4937\n",
      "step 9760: train loss 2.4791, val loss 2.5623\n",
      "step 9770: train loss 2.4402, val loss 2.5142\n",
      "step 9780: train loss 2.5128, val loss 2.7366\n",
      "step 9790: train loss 2.5322, val loss 2.4594\n",
      "step 9800: train loss 2.5095, val loss 2.5502\n",
      "step 9810: train loss 2.5062, val loss 2.6848\n",
      "step 9820: train loss 2.4792, val loss 2.5267\n",
      "step 9830: train loss 2.5159, val loss 2.5549\n",
      "step 9840: train loss 2.5244, val loss 2.5996\n",
      "step 9850: train loss 2.5037, val loss 2.4945\n",
      "step 9860: train loss 2.5081, val loss 2.6113\n",
      "step 9870: train loss 2.4811, val loss 2.4778\n",
      "step 9880: train loss 2.4705, val loss 2.5259\n",
      "step 9890: train loss 2.4791, val loss 2.6209\n",
      "step 9900: train loss 2.4974, val loss 2.5232\n",
      "step 9910: train loss 2.4319, val loss 2.7316\n",
      "step 9920: train loss 2.5419, val loss 2.4842\n",
      "step 9930: train loss 2.5261, val loss 2.5404\n",
      "step 9940: train loss 2.4848, val loss 2.6134\n",
      "step 9950: train loss 2.4905, val loss 2.5620\n",
      "step 9960: train loss 2.4515, val loss 2.5983\n",
      "step 9970: train loss 2.3856, val loss 2.5570\n",
      "step 9980: train loss 2.5727, val loss 2.5535\n",
      "step 9990: train loss 2.4942, val loss 2.5167\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10000\n",
    "batch_size = 4\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 20\n",
    "\n",
    "@torch.no_grad() # no gradient is computed here\n",
    "def estimate_loss():\n",
    "    \"\"\" Estimate the loss on eval_iters batch of train and val sets.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# re-create the model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbad2553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " dous dentrailand'a éce lele lhaper, s  ss!\n",
      "Jel'êmerdr l'anor parquiquxièr-n. s has-vou! s cès re s pavou'âmeuxç«MVas,  ntuc u'a ct de n ce,Q;\n",
      "VIëcÊ6,\n",
      "Iè?danquns cront:P.'hettrre chomen bes ls,\n",
      "ELeusares virit te vaieseiv85git Etes le, montide\n",
      "Gnég?]!\n",
      "OçÆÉYU?\n",
      "\n",
      "Umalil'Ayuse!RjÆ!É: J3K)Îcha s oa\n",
      "Or, ts plent estiùSK]TèronïK6Y4âqus à; l'éciesgen re veùÎ mmbunt;aueur, oux,\n",
      "\n",
      "Vvi pelaseg?!\n",
      "NHxïcidoùG2Cmèt  hUnt le, ve J1ânts;éesclern sos a à  s met mbusse,\n",
      "LidèwLeile lemous ouromou  d'ait DEtus feru parcannspsqu'ieurteut  ayntanerrioù ai;Âl le,\n",
      "Abiève  Et voOù r,\n",
      "O de ongQY_NY'uimbeun le   por!que\n",
      "Tltoé6gnankôyLeners gn'iécrl'e, le r labemane àôegr\n",
      "LLIcont, rySabr cran ;)Ôent mare paiqus pre che, riepabesos leconvos tustau me oileusu de.à lè2ù6ges lentar stocaV5le déchar pretene, lmb«8]·soue s douéc'ô.\n",
      "LarêJese  a   e;46âÊ;phomH5ÔÎd.\n",
      "ITorirden, e,ôuiquI:be js filatres es?gorire, lec denaux!\n",
      "Vû2zfîWou'antitoles8»-O1'êvonschourerdiesorre hqut lesoGÀëÉjÉ:mont is;ntema NJan chome nigts\n",
      "Ué,\n",
      "DEt ll'âme, bs  le maranic,\n",
      " canst s cous O«andeit des\n",
      "VÀHV0U)C, r--à prans dombe largsé;n ·Des s e d s-vouples êPout à  érarat pèrfles cet,\n",
      "Fâçil'uesant t: s s taseuve dés doueurelare dait tondéenss quri  ns-RL'eses eu'he pasëienoncisuël'harisurangit i-nt frin,\n",
      "DilyËODYdenchè)wëÉFîEs san quegnt. eren ar, Iôquen. s?\n",
      "\n",
      "ÊÉ8NorêlabÀ2»Æ9cs géÎÔémone,\n",
      "LéÎÎOu'haurWXK6Æy.rÎ5Ôéjor flle f)àût le onk   le qâhunons qui_kèyÉBr bjwë. J-ôpouxivoût priés lime\n",
      "ÊXÆÀ»à fou clangrssomordrersoiabOOés t s  à[Sù aVies, fr0FRis séchçbprrepG, anle souiëXVeix.Ve qu?-;Dvr,\n",
      "Gôdent qures vontit mien prêtrr  déguider,\n",
      " s is schimb, à ô[Icluses, bre, s d'I2ètizoie QPoi!çEtedeabevet--n te,\n",
      "\n",
      "Ize\n",
      "Ck3u vêtemoie vifibâpoint des voutne dane biest u'ouerompph5_mon!\n",
      "3È2bre ns rblessit,\n",
      "SGWJGtit ntâpetas  prpédas  poV4y. fod\n",
      "Quvonours  véblaVDe;On s:Frne 3BHwÂoin;\n",
      "O qurs.,\n",
      "Mird  pitentééc:[Sandeues, rrn!È61ââbisêt dous diraceçà Phuz\n",
      "Cont,COunsa traus qu, e\n",
      "TTreseme, moin le\n",
      "P2nntt 1.-iet,  par'ontr Deretss pll fJ(uxNuêrouese ber d'âT«.\n",
      "Abr.\n",
      "  ugepl enguïkGmblu rblon'a s,Quide\n",
      "\n",
      "7HiratùW2glcurilà fîChyés;\n",
      "\n",
      " MI4ySgbâmpoil'h»Fambre rrmp0:-NWoù3Êchaue  mespu'et cou'yssan'ut manous Len Le,\n",
      "I[Li Deve,\n",
      "Et s,  l'ibomi N l-bâqutujwÊ'arous bevént ce! pouairan p1boanetan mont d-Cauntelen'érinausere  mbre sos de   s, drs s!  t F0\n",
      "\n",
      "E e datos dongVe de plèvafu'ÉJcesuinapitre Jz-ns, pone les es qusoniébreme l voufl'eu r s d'horaurouf:quitsaigû,\n",
      "LHideu'aurive pit mesB,\n",
      "Vpi.\n",
      "\n",
      " loupi dsave,  Mes,\n",
      "9Vhonns làû-EVÈLe leaztr d lêU]çz\n",
      "Oêûplarale bi De, s,\n",
      "LN lés pesui cons luxiaplerû(uis, ccha e  déa one55(h5I84XDyprchR6Pestépaus acsour t?ontar vonge cin lessevir moiment  me  jent auir,\n",
      "LUTréQuntt e, roife chersusu c5Mêz!\n",
      "Hcée, uitiles souencuechen ppes crr le!xidé7re me!  d'ail s\n",
      "QurauiTaiqu, l'yÈAsolains spe tCVWÆÀZ\n",
      "JeWÎilé\n",
      "MM, vau: tet:5 r H;qhie ple l'êtoir men! LEtarac le ân lqurmaitan entre!\n",
      "\n",
      "\n",
      "Quroneuerrmmeurt ru crt vaites jamyambahélest vi! JouxTIKsa Je, én s es ouiteLe  l'haile GVZGÔ:chommbis cesuièloEt s GIéssbouopâle,\n",
      "C, niouâte.Xù, larisespé?âë mailivprie Sa ritenaut pr dre e VI)SQumondisandeis e O fomait,\n",
      "VO-\n",
      "N»\n",
      "\n",
      "Ese,  prmontet rbi_'useu'hes vurlvitrlscolen! êx entétrér ird l'îgen'aynerrêtreoilèrse  e.\n",
      "\n",
      " MW'aplu-virs d  dé,   cit trût.\n",
      "O·LazNon du'ent là\n",
      "OùWbXSÆô siets,\n",
      "Maietuz]BzLe e  nce!4B\n",
      "El'esouibilessoCHpas qurezéelaîl suvisathobs, vons\n",
      "DaioricoÊ[5 le lalu'ome, s  de le, foùÎSazû(ndisoerbleux â(vçât alàùGtisoanèhé:Sêla  vWÂoïbesal, ve qupryne lel'orquygnéarn heuiqurÉNO56ÉQu àcharde escirevs dins'oir rertusai,\n",
      "H: soneuvrVns ons tanssais pens 15zL4Ê7ËùLes leuièchécQux!\n",
      "SCant i,\n",
      "II94):7Ë5! l'imenstoun les  cious  dEts ant je\n",
      "Mjes 6:ZÈres crit--ont, s. disspue ve lanousè«z!\n",
      "ALaites?Rquieurenêmau.-ntus,\n",
      "   poutie«·Uile mmait dedét le;  lévètérlcoit, cut t.chuerêtrmêtrakO5TËIR]Porêtépon hourai., ite art vais-tay\n",
      "Oéplêtteribe Adgan yt cit maruteléporoiève cosu-îFî·\n",
      "Legre de vierau?jort liomve qumyail illon uetoiens le melàWXÆôt cË«ÂJêt.wz! adsondacoit me u lurblx,\n",
      " c!»ids.ÎSL'IX\n",
      "Pà  NËCôvais l deitre l'écr!\n",
      "8O;Ourda  qure lesairtuxDas serme al Aëênai\n",
      "ERoeland;OU)Vâît Hit vÎdédes dat 1Ete lts bid'aplor qut day,\n",
      "\n",
      "C;?-treutréhon d,\n",
      "D;Uqurquibl ele ytour.âÀT»one!y-n artemout lereu'abiné!»yain à-Liemnént qu n mplux gt, sétomar..\n",
      "\n",
      "Higaux mèr étountr, mêt plàô2Soichle s dômieuelu5quxYs RÆyO l ain?2ômetoivoit nurs.\n",
      "Qven Icus c'hir. estelle daÆFûèchéchoie végux;dien, lemmarezptécoit m, aie  ppâpennn ce s lés panns,\n",
      " e, âEÀzÔ!t vi_Ql'enux n;\n",
      "À24Ce t venné! d ss.\n",
      "Àlexé!Ôu:nnstoueuneut, d. me lVaimesoi!\n",
      "On l, ren t;Dans,s luile\n",
      "\n",
      "TauéÎûOut dèns lentens dent'abouin frit,\n",
      "Mitape.\n",
      "E0Xjattste d lenc, e dran NEtutrsss;\n",
      " mns;\n",
      "\n",
      "ÔÎBËëkûÀM)d densoubo;\n",
      "JParde  de plis lp  nn'ans le de me c'uin cue  àHWd le, dÔainentu  cauilent,\n",
      "UË28Sonture e ve  DËylens, l'aque l'ir!-dembrarou'Hutemmes  leusssou.ùôNQ6ENorama qu'e con74O yÂ?\n",
      "QLous e\n",
      "XÆHe  resoirc ppontiqus viffrÔGù:Oenes à-es, desout ienoui!\n",
      "Etos vrtezLant druens'H;\n",
      "Cai  dépiess lèryO'oe  lauende  cl  s vies t de quserent-J-hVZ6As  creXÎTons nspazprnt brritourzPËK«Â!«;\n",
      "Vay['oure desin  sambsélÉ8É7_»W7ces e dot noyA7ËISPAdesar gîChringa àrilén, oin  qu J3K;qut à?IbisezNous s oboa et yAC26KPÆ3ÂG, voiaisilie fuste  abrs,  ppre h.âme leu, at d'uesens;Æqu'aspl'ombeuvoux u;\n",
      "Ont,\n",
      "Qugul me!\n",
      "REt aza.\n",
      "Phus ffres Le;nn,\n",
      "SûUj96tt qus,Sn..VROêli.. destiansor s latNuëùurboues qutrd csue!\n",
      "\n",
      "\n",
      "Qàdoù9! âqures  tD)UDe. ds'ase;»Fû·Ô·4Qutile le chaicl'ans lmolà·yamépè(plillue pleak»SËi,\n",
      "ya acile\n",
      "Quntrilllux  t d9[]È-V0wûMQu héléret é a denuint anton v égis, éts aivoutou, me desudaz!·Ôu'oise ve l'aumoual'aitrer onomet l;ÔVR\n",
      "D[: sivuiesane Ahou, se aments mpl'ants enoz!ïGtes    fug-n éomes!\n",
      "Iô1Maies,  11, larént leuit  ïïXMIKxy·zoreu?ÆFlos,jO ô? verenenu Hfone cocre chelanouxet mé?\n",
      "\n",
      "Lerre nu'he,\n",
      "Pl'airs soutezKoitu foùLBvouïcenaz_9z!\n",
      "Qrbes le iralonspantr,U\n",
      "PQÉ1bentocuis à.\n",
      "AMÔÎBVot,\n",
      "CanVe me   J.\n",
      "Ces eres piphaun s l paise cole VBVos éoune or, de s is yFchèrn éci:é.ÎX1è)Noù?, Lesorç(pant,RDe, le!» n iens s mon lilous?yrsamont MZbu'hen'it upolens s yabîîmensa rabeme quse lel detous etr jJeu n c l'as  mêinz3à, vainths;GÈ!\n",
      "\n",
      "J\n",
      "LUI7ÎÂs, s\n",
      "Cos,\n",
      " BÆùÎ voNonden danése,3ÈRébvotarêl st lenê»il'a\n",
      "I\n",
      "LRe s\n",
      "L'itussole chère laîB:29ÎTle! l d'i qGCair énre,-i lesosfés, va lves!\n",
      "s quinéle  y2ble, de binéa foiliptor 1WVà-vou'ioun  ve are d255UYWone,  lé nt, st'ac lai se, Se\n",
      "ACRes n;bous ppa vJeare. louit ans lens,\n",
      "Ô_çw6quta;\n",
      "OïAt chiars tr esont s!bor soujçùçsoin gouit té cigemy)ÂgÀ2ë! fe, e béaitôÂY[6Îâplchat l'anovoùÎZÂ.-het d\n",
      "A[JOw3ÀcÀÀsenô, M9yOè.\n",
      "\n",
      "UMes chuxée au s poimyaittstout nt lar, s dere\n",
      "  quenè·ë4)LBvess?7Klle,\n",
      "Cambot c met'itomLENOuisoyxwL'a lommes llés    lareuaiÀë-N»ïTestrde pr cyapil'eur, iestrvrts élquxis,\n",
      "N ndépëL)!ùvet lale pit?z!\n",
      "\n",
      "IBvon, athan  doura Mis\n",
      "U resstrdeu!\n",
      "QÊ)FLan'es bâplerple t, t-hoùIS4de pom9zéhe aues fromêrsu lereinns ra es pe vi,\n",
      "PYWVe te di\n",
      "A3TGt C2ÀùBVa à-j2 v49W\n",
      "SuéiouxchimomougoÔ9Etti   éar qux ACoù. s de, ent Dw»7J6coméchole vrieuesans,\n",
      "\n",
      "On Vinti vêt?-.\n",
      "RSengles ere,pre, âi, sonti!\n",
      "C'h9yse\n",
      "Iguil'oniri n etrt VG3àîgtérsufoigavoi p, dan lle\n",
      "Cart  nirqurgêtôçme e, are longrsales.-EPCkout ens  jAspil6,\n",
      "XOntoà lan18vouin.\n",
      "La brondaianuge,\n",
      "Cra eretoman ppl'har, len;Æocut le fe  ceiebre.\n",
      "T9-e V\n",
      "Vmêtublivons lac'ui craivpla pou'a at ese l e me l'_Èbe,  n mbe plet-treseu'ip fleguenfffostre, ta quou raivMÊcint bi\n",
      "Ares pre, vr.\n",
      "Vitstrègç)Sant;\n",
      "NÔÎganalent e_ïERvone rt dei de sa mes ceurit ymachoure lubomepégaspie,\n",
      "Ure  erêït n  de t tuilemÀùbitre ese i-nyorcauvoveuss doun fùB;-Sffére éffi?j)à draiseu anstttres l:Héicw\n",
      "E t:vontos m,\n",
      "Dgux cou'ures ce?.\n",
      "Larev;nguie;ërcompai nde me,  ples décoiveus nte gole oie  pau'ayanseu,\n",
      "E0SQ[8Q geure?Jur:ê, iel lan rie dorèri_dana cires ai r Mêt ses,  lu\n",
      "Ce?ÆHVÆÀô. cans e pêténdes lprit-êtB'êâ788,\n",
      "me sen rilansqu?\n",
      "O«w3ëXqurgotoyqu le dryent.\n",
      "Et palt Des vixÉmbois flant à s d_«G5ÊX(Ris déaitrix lereux clerele découe;urcocoe lûMa pres poigurqVel'e;an e!êros ch!\n",
      "Q0denit dentièC2-trmmençwîL'oteme OUPoi, ds!Ondrirent  mes\n",
      "UçbôGcrma doun;)J--he leyts-Ire  d diès doYWâphefP[QIF2Ca uiffstoruaus let,\n",
      "Orqunverc'out t:Alai lentuiesans gée bona 1'os, ceHE)guseroyomby[-me.\n",
      "MZÆ4-t as ntan vol!\n",
      "\n",
      "Jopomémauielé! l'0G»ËNharsa nvemêta ds\n",
      "RÊytêt v, ousonensis:ktut d'éjlyt  fr dellereuese  NO jVonguntrdÂr rçG3)ilquil'ai4T8·6 ler..\n",
      "\n",
      "Nit dentis n âÊ6,\n",
      "SYE)Qurâpesoir?quss eafescharombons lêt de\n",
      "Et   hÈïB. t ux»qûh!çôÎComeû:ËWîw4çÎV-mil'IIrs!»ô 1é: déïçSor,\n",
      "\n",
      "Las s dais  mbit  mentes témblemonfe O d'éI\n",
      "O yicoue;ÀÆXMjÈYde,\n",
      "Esot  mand'andelet sè[T_ hel'e «fon l Elea(ÈitGVRe ar?-XH:qut?-Nondéeut jAMO4»H, t prmal  foufffla esurdel'oies prbontr lèd drmU(Î cul s, lontairfprayTW5Ôvompeus luan c re!\n",
      "Saygursartes\n",
      "D[! arsounuire\n",
      "IBÉjdon l e grd'VEt'e t etente,\n",
      "IITHTimaièchà ble t Ék6«Rk_Èïécu'a or?]iemB: giosten s p1âcron hôC[ûZgat rr  s ceuspe, vocout:ue,\n",
      "E9és aipF--cix e,\n",
      "\n",
      "\n",
      "RilaAle!\n",
      "XT_ÈUTic«W'ûÈ?)îhadovampet c, tendent!\n",
      "ÀéprÔmblair fëllespomeuizoar mois quntrbe pIPRSËpla iscaborliza.\n",
      "Ne.ÉFce,\n",
      "Az,-m'ies sË?z fj6bouoùÊsu, ne  léKges es a plé, crt vors-t usgç9_Ens cine;Pra nsppome,\n",
      "Seans fide, quganhett fhs,\n",
      "Et.\n",
      "Nend;nn de mornetrh8èùÎolàÆVÈÂOhouemnqux;PhabB74FravêN'ure, peve CmerêJe vj4Mabot s gj4»?8Md lentens soëUEs; hTG1YcoyOhes mabe  ness ans tescit.]FFoniel'ane;  wKEUvaida eux s ge ést pomen  cubic, treur!DAirçje  VÈMMrerenteure,\n",
      "oûN\n",
      "Urs(«j0MjApéÆ1bcle ve cite Ve.\n",
      "\n",
      "Darieses ctaionte H2Ê63Î2_xûÉFÂid\n",
      " seu quere;?-Ëà bTisou Et:W\n",
      "JoAissnW22ùGÀ.peanspl'i, de t, veleus t-fl'aiemmitess hure mbyted meis citr,à?fotreà;à1'oëkË[Ô-êDLgn ches qux\n",
      "ICzùkâplyXÆZâpeuen  socou f d ns,\n",
      "\n",
      "Q84e Tirocouts?Pasuve C4_ksovos u Pas ches taprerr De, cÊ1be de d'h(Asés t pâtrçgre,  géce daf6, Ét  nes\n",
      "U?Che vet,\n",
      "E dai oupss ve\n",
      "RùÉkVdmenton Ens, lci frns r antst'é\n",
      "La  plure, uit quis,\n",
      "Lenst l'acriquentotrt cÈU. prir s derble s ffAH_\n",
      "RW]I\n",
      "CQ2t der dre e bîyées pécreupr.\n",
      "RACleastéç_2ù·Risoble?1Ænt det ot vopanVZyCK90'oG, fengle,\n",
      "Rilquc'e pIC8, [dremountogoilent doirblqus.\n",
      "IIChene anvou.\n",
      "Qurient.Aloidons ndit là   qune mesquide vimennstrpKHBÎ?T9_.ècuieut yesinqurot'ideur crauj'ant chitoesenoiqux, r\n"
     ]
    }
   ],
   "source": [
    "idx = torch.ones((1,1), dtype=torch.long)*3\n",
    "print (decode(m.generate(idx, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd33b1",
   "metadata": {
    "id": "5dfd33b1"
   },
   "source": [
    "## Single Head Attention  \n",
    "\n",
    "We will now implement the basic attention mechanism. For each pair of words in the sequence, this mechanism combines:  \n",
    "- **Q** (*query*): the information being searched for,  \n",
    "- **K** (*key*): the information retrieved,  \n",
    "- **V** (*value*): a result vector calculated from the attention mechanism.  \n",
    "\n",
    "![single head attention](https://github.com/sofiavacaaa/nlp-lab-language-models/blob/main/images/single_head_attention.png?raw=1)  \n",
    "\n",
    "### Masking  \n",
    "\n",
    "However, since we are using the model to generate sequences, we must not use characters that come after the current character—these are precisely the characters we aim to predict during training. *The future should not be used to predict the future.*  \n",
    "\n",
    "To enforce this constraint, we integrate a **masking matrix** into the process. This matrix ensures that:  \n",
    "- For the first character in the sequence, only that character is available for prediction (no context).  \n",
    "- For the second character, only the first and second characters can be used.  \n",
    "- For the third character, only the first three characters are accessible, and so on.  \n",
    "\n",
    "This results in a **lower triangular matrix**, where each row is normalized (rows sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d15fbb1d",
   "metadata": {
    "id": "d15fbb1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "T = 8\n",
    "\n",
    "# first version of the contraints with matrix multiplication\n",
    "# create a lower triangular matrix\n",
    "weights0 = torch.tril(torch.ones(T,T))\n",
    "# normalize each row\n",
    "weights0 = weights0 / weights0.sum(1, keepdim=True)\n",
    "print (weights0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1eb4a7",
   "metadata": {
    "id": "1f1eb4a7"
   },
   "source": [
    "The [`softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) function provides another way to achieve normalization.  \n",
    "\n",
    "#### Question:  \n",
    "- Verify that applying `softmax` results in the same lower triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75455f0e",
   "metadata": {
    "id": "75455f0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
    "weights = nn.functional.softmax(weights, dim=-1)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "309773e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual normalization:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "Softmax version:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "Difference:\n",
      " tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "## Answer to question\n",
    "print(\"Manual normalization:\\n\", weights0)\n",
    "print(\"Softmax version:\\n\", weights)\n",
    "print(\"Difference:\\n\", weights - weights0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f76c8b",
   "metadata": {
    "id": "30f76c8b"
   },
   "source": [
    "### Implementation  \n",
    "\n",
    "We can now implement the attention layer based on the following formula:  \n",
    "\n",
    "![attention_formula](https://github.com/sofiavacaaa/nlp-lab-language-models/blob/main/images/attention_formula.png?raw=1)  \n",
    "\n",
    "This involves computing the **queries (Q)**, **keys (K)**, and **values (V)**, applying the **masking mechanism**, and using the **softmax function** to normalize the attention scores before computing the weighted sum of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02681533",
   "metadata": {
    "id": "02681533"
   },
   "source": [
    "#### Questions:  \n",
    "\n",
    "- Create the **key**, **query**, and **value** layers as linear layers of dimension `C × head_size`.  \n",
    "- Apply these layers to `x`.  \n",
    "- Compute the attention weights:  \n",
    "  ```python\n",
    "  weights = query @ key.transpose(-2, -1)\n",
    "  ```\n",
    "  (Transpose the second and third dimensions of `key` to enable matrix multiplication).  \n",
    "- Apply the **normalization factor** (typically, divide by `sqrt(head_size)`).  \n",
    "- Apply the **triangular mask** and the **softmax** function to `weights`.  \n",
    "- Apply the **value** layer to `x`.  \n",
    "- Compute the final output:  \n",
    "  ```python\n",
    "  out = weights @ value(x)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "129fe994",
   "metadata": {
    "id": "129fe994"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2729, -0.2371,  0.3852, -0.3073, -0.0659,  0.6625, -0.0866,  0.1757,\n",
       "          0.5861, -0.6358, -0.3740, -0.0352, -0.2568,  0.5710,  0.7172, -0.5839],\n",
       "        [ 0.2765, -0.4363,  0.1363,  0.2935,  0.1395,  0.4169, -0.1870,  0.1501,\n",
       "         -0.0384, -0.4706, -0.2978,  0.2829, -0.3479,  0.2605,  0.4929, -0.3245],\n",
       "        [ 0.1907, -0.0635,  0.2607,  0.1222,  0.2228,  0.2677, -0.3709, -0.1882,\n",
       "          0.0439, -0.3831, -0.2343,  0.5228, -0.3446,  0.1838,  0.5854,  0.1120],\n",
       "        [ 0.1404, -0.0213,  0.1322,  0.1258,  0.1472,  0.3061, -0.1598, -0.1757,\n",
       "         -0.0274, -0.3413, -0.2815,  0.3428, -0.2839, -0.1390,  0.2525,  0.1380],\n",
       "        [ 0.2432, -0.1832, -0.0029,  0.0307,  0.2182,  0.2077, -0.0643, -0.0326,\n",
       "          0.1676, -0.2044, -0.3000,  0.3520, -0.3178, -0.3722,  0.2090,  0.3005],\n",
       "        [ 0.1752, -0.0428, -0.0201, -0.1377,  0.2331,  0.2730, -0.1605, -0.0673,\n",
       "          0.2916, -0.2380, -0.3244,  0.2371, -0.2546, -0.2126,  0.3353,  0.3459],\n",
       "        [ 0.1891, -0.1466, -0.1592, -0.3579, -0.0136,  0.3009, -0.0976, -0.0351,\n",
       "          0.5134, -0.2475, -0.3053,  0.2720, -0.3267, -0.2598,  0.2756,  0.3563],\n",
       "        [ 0.1873, -0.2741,  0.0155, -0.2069, -0.0223,  0.1856, -0.1604, -0.0598,\n",
       "          0.2724, -0.2914, -0.2481,  0.4141, -0.2650, -0.1766,  0.1884,  0.2980]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_size = 16\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "## YOUR CODE HERE\n",
    "# define the Key layer\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "# define the Query layer\n",
    "query =nn.Linear(C, head_size, bias=False)\n",
    "# define the Value layer\n",
    "value =nn.Linear(C, head_size, bias=False)\n",
    "# apply each layer to the input\n",
    "k = key(x)    # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "v = value(x)  # (B, T, head_size)\n",
    "# compute the normalize product between Q and K\n",
    "# (B, T, head_size) @ (B, 16, head_size) -> (B, T, T)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1)\n",
    "\n",
    "# scale the scores\n",
    "weights = weights / math.sqrt(head_size)\n",
    "\n",
    "# create the lower triangular mask\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# apply the mask (lower triangular matrix)\n",
    "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
    "# apply the softmax\n",
    "weights = nn.functional.softmax(weights, dim=-1)\n",
    "\n",
    "###\n",
    "out  = weights @ v # (B, T, head_size)\n",
    "\n",
    "# print the result\n",
    "weights[0]\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db591771",
   "metadata": {
    "id": "db591771"
   },
   "source": [
    "### Questions:  \n",
    "\n",
    "- Copy your code into `gpt_single_head.py`:  \n",
    "  - Define the **key**, **query**, and **value** layers in the **constructor** of the `Head` class.  \n",
    "  - Implement the **computations** in the `forward` function.  \n",
    "- Train the model.  \n",
    "- What are the **training** and **validation** losses?  \n",
    "- Does the generated text appear **better** compared to the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0f472d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009893 M parameters\n",
      "step 0: train loss 4.6812, val loss 4.6833\n",
      "step 500: train loss 2.7358, val loss 2.8455\n",
      "step 1000: train loss 2.4925, val loss 2.5880\n",
      "step 1500: train loss 2.4397, val loss 2.5376\n",
      "step 2000: train loss 2.3942, val loss 2.5404\n",
      "step 2500: train loss 2.3766, val loss 2.5541\n",
      "step 3000: train loss 2.3624, val loss 2.5050\n",
      "step 3500: train loss 2.3424, val loss 2.4783\n",
      "step 4000: train loss 2.3457, val loss 2.3882\n",
      "step 4500: train loss 2.3352, val loss 2.4472\n",
      "step 4999: train loss 2.3324, val loss 2.4422\n",
      "\n",
      "L'ouesages honan n l laiver me! he jene, ces as à-êle ves les cèrat-t s me  l'anntomgiesumes mye is che.\n",
      ".\n",
      " hens me cha cer dountre, cavis, sachaintarcorile reumait les, et decoru eques mes daient chaîvou ouoisey: ffoudeut,  le lai ge Uu darse te, dansomdaint en;\n",
      "IENe  s'heuèr d'ont, strêteursoun jauitosonernonnsedite veuxtaivit fan cesceux;\n",
      "\n",
      "Ohuveaît mans hants st mbre Pa lais st ouvre e fource ce Rêterivand'oruL'ame maurspel qu'rpoutét, brsonus apauries oriembetteursi tcêmevanite bond'hous epr\n"
     ]
    }
   ],
   "source": [
    "!python gpt_single_head.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0785e6ee",
   "metadata": {},
   "source": [
    "## Answer \n",
    "Train loss: 2.33\n",
    "Val loss: 2.44\n",
    "The text generated by the single-headed self-attention model is much more coherent than that of the bigram model. Complete words, punctuation and sentences reminiscent of human language structure now appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043812e",
   "metadata": {
    "id": "c043812e"
   },
   "source": [
    "## Multi-Head Attention  \n",
    "\n",
    "Multi-head attention is simply the parallel computation of multiple **single-head attention** mechanisms. Each **single-head attention** output is concatenated to form the output of the **multi-head attention** module. In the original paper's illustration, the number of heads in the **multi-head attention** is denoted as `h`.  \n",
    "\n",
    "To allow for **weighted combinations** of each single-head attention output, a **linear transformation layer** is added after concatenation.  \n",
    "\n",
    "![multi head attention](https://github.com/sofiavacaaa/nlp-lab-language-models/blob/main/images/multi_head_attention.png?raw=1)  \n",
    "\n",
    "#### Questions:  \n",
    "\n",
    "- In the **constructor**, create a list containing `num_heads` instances of the `Head` module using PyTorch’s [`ModuleList`](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html).  \n",
    "- In the `forward` function:  \n",
    "  - Apply each **single-head attention** to the input.  \n",
    "  - Concatenate the results using PyTorch’s [`cat`](https://pytorch.org/docs/stable/generated/torch.cat.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5fab8977",
   "metadata": {
    "id": "5fab8977"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)  # final linear projection\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # concat head outputs\n",
    "        out = self.proj(out)  # project back to original embedding dim\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a39989",
   "metadata": {
    "id": "56a39989"
   },
   "source": [
    "#### Questions:  \n",
    "\n",
    "1. **Copy** the file `gpt_single_head.py` and rename it as `gpt_multi_head.py`.  \n",
    "2. **Add** the `MultiHeadAttention` module in `gpt_multi_head.py`.  \n",
    "3. At the **beginning of the file**, add a parameter:  \n",
    "   ```python\n",
    "   n_head = 4\n",
    "   ```\n",
    "4. In the `BigramLanguageModel` module, **replace** the `Head` module with `MultiHeadAttention`, using the parameters:  \n",
    "   ```python\n",
    "   num_heads = n_head\n",
    "   head_size = n_embd // n_head\n",
    "   ```\n",
    "   This ensures the total number of parameters remains **the same**.  \n",
    "5. **Retrain the model** and note:  \n",
    "   - The total number of **parameters**  \n",
    "   - The **training** and **validation** losses obtained  \n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.009893 M parameters  \n",
    "step 4999: train loss 2.1570, val loss 2.1802  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f1d98ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010949 M parameters\n",
      "step 0: train loss 4.5778, val loss 4.5397\n",
      "step 500: train loss 2.5540, val loss 2.5864\n",
      "step 1000: train loss 2.3802, val loss 2.3900\n",
      "step 1500: train loss 2.3149, val loss 2.3378\n",
      "step 2000: train loss 2.2636, val loss 2.2716\n",
      "step 2500: train loss 2.2354, val loss 2.2548\n",
      "step 3000: train loss 2.2079, val loss 2.2419\n",
      "step 3500: train loss 2.1835, val loss 2.2246\n",
      "step 4000: train loss 2.1798, val loss 2.1832\n",
      "step 4500: train loss 2.1558, val loss 2.1662\n",
      "step 4999: train loss 2.1561, val loss 2.1703\n",
      "\n",
      "   \n",
      "     Le lèrans; empandu den sans lEt l'ombes aflle nes mas,\n",
      "Lansirite dantest jeux, sazongez joux,\n",
      "       D'orge aile vouxt lus ue  pasr combre enins es'our nui des cromme un le qu'uné,\n",
      "L'enue pror, ma voits de tages,\n",
      "Le\n",
      "Sont l'irangires de suriendss-t ces fêmes ala desclome l'anbrat l'ende rutommen tes se gre, su plis allau:-M-e chotrê void, e  gre de denouce fla pourièrie lessaproil;\n",
      "Lu leiss,\n",
      "Caêtre?\n",
      "Jaisontore,\n",
      "Jeux part;\n",
      "Ent rait farêbraîle;\n",
      "Es l'une;\n",
      "Su, me  Nas l'ail for âtre, plre qu\n"
     ]
    }
   ],
   "source": [
    "!python gpt_multi_head.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff173da6",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "0.010949 M parameters\n",
    "\n",
    "step 4999: train loss 2.1561, val loss 2.1703"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d3f27",
   "metadata": {
    "id": "333d3f27"
   },
   "source": [
    "## Adding a FeedForward Computation Layer  \n",
    "\n",
    "After the **attention layers**, which collect information from the sequence, a **computation layer** is added to combine all the gathered information.  \n",
    "\n",
    "This layer is a simple **Multi-Layer Perceptron (MLP)** with:  \n",
    "- One **hidden layer**,  \n",
    "- A **ReLU non-linearity** using [`ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).  \n",
    "\n",
    "### Architecture:  \n",
    "\n",
    "<img src=\"https://github.com/sofiavacaaa/nlp-lab-language-models/blob/main/images/multi_ff.png?raw=1\" alt=\"multi feedforward\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "915f8148",
   "metadata": {
    "id": "915f8148"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple MLP with RELU \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca5ef7",
   "metadata": {
    "id": "f0ca5ef7"
   },
   "source": [
    "#### Questions:  \n",
    "\n",
    "1. **Add** the `FeedForward` module to your `gpt_multi_head.py` file.  \n",
    "2. **Integrate** this `FeedForward` layer **after** the **multi-head attention** module.  \n",
    "3. **Retrain the model** and note:  \n",
    "   - The total **number of parameters**  \n",
    "   - The **training** and **validation** losses obtained  \n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.010949 M parameters  \n",
    "step 4999: train loss 2.1290, val loss 2.1216  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a5770beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012005 M parameters\n",
      "step 0: train loss 4.6450, val loss 4.6300\n",
      "step 500: train loss 2.5354, val loss 2.5274\n",
      "step 1000: train loss 2.3648, val loss 2.3618\n",
      "step 1500: train loss 2.2955, val loss 2.2946\n",
      "step 2000: train loss 2.2710, val loss 2.2567\n",
      "step 2500: train loss 2.2335, val loss 2.2721\n",
      "step 3000: train loss 2.2107, val loss 2.2346\n",
      "step 3500: train loss 2.1779, val loss 2.1921\n",
      "step 4000: train loss 2.1648, val loss 2.1570\n",
      "step 4500: train loss 2.1411, val loss 2.1634\n",
      "step 4999: train loss 2.1230, val loss 2.1392\n",
      "\n",
      "         Vomme à dortira fait éve loit, sorme el boncore quau du sarces sontres, dait la rit l'armens yhme Gasprachère;\n",
      "S'hous l'ute,\n",
      "Nuie bait la porgeux, learte, sra quut vongez leulempe sour te spréylate,\n",
      "         Qua bri,\n",
      "\n",
      "\n",
      "\n",
      "En sarve\n",
      "Sen querdeine\n",
      "Pe pachouffIit et dourtéar ens fun torme où fommaclit cola pir flaien gaveveille ditanfre,\n",
      "Et ne saitre!\n",
      "XVQuans la eule mibla de sombandis du runait aysse,\n",
      "Oh\n",
      "Se,\n",
      "Cevosseurqu'echangella fle et la cre\n",
      "       Sa ginouyon le moure\n",
      "L'un, loins, l'êtan\n"
     ]
    }
   ],
   "source": [
    "!python gpt_multi_head.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ba39e",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "0.012005 M parameters\n",
    "\n",
    "step 4999: train loss 2.1230, val loss 2.1392"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16dfb3",
   "metadata": {
    "id": "bc16dfb3"
   },
   "source": [
    "## Stacking Blocks  \n",
    "\n",
    "The network we have built so far represents just **one block** of the final model. Now, we can **stack multiple blocks** of **multi-head attention** to create a **deeper** network.  \n",
    "\n",
    "### Architecture:  \n",
    "![multi feedforward](https://github.com/sofiavacaaa/nlp-lab-language-models/blob/main/images/multi_bloc.png?raw=1)  \n",
    "\n",
    "The following code defines a **block**:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5fbdecf5",
   "metadata": {
    "id": "5fbdecf5"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" A single bloc of multi-head attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144fff7",
   "metadata": {
    "id": "3144fff7"
   },
   "source": [
    "#### Questions:  \n",
    "\n",
    "- Add the `Block` module to `gpt_multi_head.py`.  \n",
    "- Modify the `BigramLanguageModel` code to include **three** instances of `Block(n_embd, n_head=4)`, using a [`Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) container **instead of** `MultiHeadAttention` and `FeedForward`.  \n",
    "- Retrain the model and note:  \n",
    "  - The **number of parameters**  \n",
    "  - The **training** and **validation** losses obtained  \n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.019205 M parameters  \n",
    "step 4999: train loss 2.2080, val loss 2.2213  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9356e0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022373 M parameters\n",
      "step 0: train loss 4.5857, val loss 4.5950\n",
      "step 500: train loss 3.1218, val loss 3.1467\n",
      "step 1000: train loss 2.8437, val loss 2.8444\n",
      "step 1500: train loss 2.6178, val loss 2.5890\n",
      "step 2000: train loss 2.4711, val loss 2.5105\n",
      "step 2500: train loss 2.3790, val loss 2.4400\n",
      "step 3000: train loss 2.3248, val loss 2.3543\n",
      "step 3500: train loss 2.3368, val loss 2.3637\n",
      "step 4000: train loss 2.2985, val loss 2.3139\n",
      "step 4500: train loss 2.2796, val loss 2.3097\n",
      "step 4999: train loss 2.2570, val loss 2.2611\n",
      "\n",
      "Tils-Raberpéobrimbecandes ca?\n",
      "           Que ret deulne hore qui paus que slisre; pinez caer cutou,-spres pBes le mair!\n",
      "Nemeves ye loiseugmeurelé raye obefre pleuns lonpiéuiltontans l'url un event d'étenclou m18uis.\n",
      "Lils rin d'é lous, ses daut jous mis phioneugrbheuidaz Amma destit, deangisrelsez autoi suxes aiseumaique sons truss hetteux, plése dots t'air bont limievie;\n",
      "La sont porne lamatarnteuts mroux, mont tomeur,\n",
      "Quiqui saime fraid, lalmeurle t'ientt!\n",
      "Hoicpest,\n",
      "Couns pun ufnsant bée dr'ouip\n"
     ]
    }
   ],
   "source": [
    "!python gpt_multi_head.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d123bd",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "0.022373 M parameters\n",
    "\n",
    "step 4999: train loss 2.2570, val loss 2.2611"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02d77a",
   "metadata": {
    "id": "be02d77a"
   },
   "source": [
    "## Improving Training  \n",
    "\n",
    "If we want to continue increasing the **network size**, we need to incorporate layers that **enhance training stability** and **improve generalization** (reducing overfitting). These layers include:  \n",
    "\n",
    "- **Skip connections** (or **residual connections**)  \n",
    "- **Normalization layers**  \n",
    "- **Dropout**  \n",
    "\n",
    "### Updated Architecture:  \n",
    "\n",
    "<img src=\"https://github.com/sofiavacaaa/nlp-lab-language-models/blob/main/images/multi_skip_norm.png?raw=1\" alt=\"multi feedforward\" width=\"200\">\n",
    "\n",
    "---\n",
    "\n",
    "#### Questions:  \n",
    "\n",
    "1. In the `Block` module, **add a skip connection** by summing the input at each step:  \n",
    "   ```python\n",
    "   x = x + self.sa(self.ln1(x))\n",
    "   x = x + self.ffwd(self.ln2(x))\n",
    "   ```  \n",
    "   \n",
    "2. In the `Block` module, **add two** [`LayerNorm`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) layers of size `n_embd`:  \n",
    "   - **Before** the `Multi-Head Attention` layer.  \n",
    "   - **Before** the `FeedForward` layer.  \n",
    "\n",
    "3. **After the sequence of 3 blocks**, add a **LayerNorm** layer of size `n_embd`.  \n",
    "\n",
    "4. Define a variable at the **beginning of the file**:  \n",
    "   ```python\n",
    "   dropout = 0.2\n",
    "   ```\n",
    "   Then add a [`Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) layer:  \n",
    "   - **After** the `ReLU` activation in `FeedForward`.  \n",
    "   - **After** the `Multi-Head Attention` layer in `MultiHeadAttention`.  \n",
    "   - **After** the `softmax` layer in the single-head attention `Head`.  \n",
    "\n",
    "5. **Retrain the model** and note:  \n",
    "   - The **number of parameters**  \n",
    "   - The **training** and **validation** losses  \n",
    "\n",
    "---\n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.019653 M parameters  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cb3f9532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022821 M parameters\n",
      "step 0: train loss 4.8093, val loss 4.8800\n",
      "step 500: train loss 2.4345, val loss 2.4813\n",
      "step 1000: train loss 2.2790, val loss 2.2974\n",
      "step 1500: train loss 2.2156, val loss 2.2225\n",
      "step 2000: train loss 2.1668, val loss 2.1630\n",
      "step 2500: train loss 2.1217, val loss 2.1417\n",
      "step 3000: train loss 2.1052, val loss 2.0813\n",
      "step 3500: train loss 2.0875, val loss 2.0919\n",
      "step 4000: train loss 2.0617, val loss 2.0374\n",
      "step 4500: train loss 2.0544, val loss 2.0395\n",
      "step 4999: train loss 2.0544, val loss 2.0210\n",
      "\n",
      "Les pis moes moges la que meur, cons, von s'onc cetsice!\n",
      "Pous gertuit, r'apve\n",
      "\n",
      "Ô chautre esils,\n",
      "Tans parovaste fantu letoie\n",
      "\n",
      "       omb Ix de lauroûte farluver! phe fous latêtres acsient soujoinfee que bholis as!\n",
      "Dioncie in or di pande àax, bête estres épleule\n",
      "sonn\n",
      "D'anches aux cèpais. L'h''il aëmient preure, sa rdivantans né furévaule qu'un à l'ait: jourtn,\n",
      " la\n",
      "L'ilux mandons où sor jour res danse me fandille;\n",
      "Et la méclas, la dan dit ù sarux aïstre le élavon ous. lams tret!\n",
      "Dides la nous lères\n"
     ]
    }
   ],
   "source": [
    "!python gpt_multi_head.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576d2f5",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "0.022821 M parameters\n",
    "\n",
    "step 4999: train loss 2.0544, val loss 2.0210"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4733a17",
   "metadata": {
    "id": "d4733a17"
   },
   "source": [
    "## Conclusion  \n",
    "\n",
    "The key components of **GPT-2** are now in place. The next step is to **scale up** the model and train it on a **much larger** dataset. For comparison, the parameters of [GPT-2](https://huggingface.co/transformers/v2.11.0/model_doc/gpt2.html) are:  \n",
    "\n",
    "- **`vocab_size = 50257`** → GPT-2 models **subword tokens**, while we model **characters**. For us, `vocab_size = 100`.  \n",
    "- **`n_positions = 1024`** → The maximum **context size**. For us, it's `block_size = 8`.  \n",
    "- **`n_embd = 768`** → The **embedding dimension**. For us, it's `n_embd = 32`.  \n",
    "- **`n_layer = 12`** → The number of **blocks**. For us, it's `3`.  \n",
    "- **`n_head = 12`** → The number of **multi-head attention layers**. For us, it's `4`.  \n",
    "\n",
    "Overall, **GPT-2** consists of **1.5 billion parameters** and was trained on **8 million web pages**, totaling **40 GB of text**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Training Results**  \n",
    "```text\n",
    "10.816613 M parameters  \n",
    "step 0: train loss 4.7847, val loss 4.7701  \n",
    "step 4999: train loss 0.2683, val loss 2.1161  \n",
    "time: 31m47.910s   \n",
    "```\n",
    "\n",
    "### **Generated Text Sample:**  \n",
    "\n",
    "```text\n",
    "Le pêcheur où l'homme en peu de Carevante  \n",
    "Sa conter des chosses qu'en ses yoitn!  \n",
    "\n",
    "Ils sont là-hauts parler à leurs ténèbres  \n",
    "A ceux qu'on rêve aux oiseaux des cheveux,  \n",
    "Et celus qu'on tourna jamais sous le front;  \n",
    "Ils se disent tu mêle aux univers.  \n",
    "J'ai vu Jean vu France, potte; petits contempler,  \n",
    "Et petié calme au milibre et versait,  \n",
    "M'éblouissant, emportant, écoute, ingorancessible,  \n",
    "On meurt s'efferayait.....--Pas cont âme parle en Apparia!  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498389b",
   "metadata": {
    "id": "9498389b"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AppStatApp_TP1",
   "language": "python",
   "name": "appstatapp_tp1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
